{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 7: LSTM Deep Learning Model Training (PyTorch)\n",
        "\n",
        "## Goal\n",
        "Train a Long Short-Term Memory (LSTM) neural network using PyTorch to predict BIST-100 price direction, leveraging temporal patterns that traditional ML models might miss.\n",
        "\n",
        "## Why LSTM?\n",
        "- **Temporal Dependencies**: LSTM networks excel at capturing long-term dependencies in time series data\n",
        "- **Sequence Learning**: Can learn complex patterns across multiple time steps\n",
        "- **Non-linear Relationships**: Deep learning can capture non-linear relationships between features\n",
        "- **Lagged Macro Features**: Incorporates 1-month and 3-month lagged inflation and interest rates\n",
        "\n",
        "## Why PyTorch?\n",
        "- **Python 3.14 Compatible**: Works with the latest Python versions\n",
        "- **Flexible**: Easy to customize model architecture\n",
        "- **Performance**: Efficient training with GPU support (if available)\n",
        "\n",
        "## Models\n",
        "- **LSTM Neural Network (PyTorch)**: Multi-layer LSTM with dropout and batch normalization\n",
        "- **Comparison**: Compare with XGBoost baseline\n",
        "\n",
        "## Evaluation Metrics\n",
        "- Accuracy, Precision, Recall, F1-Score\n",
        "- Confusion Matrix\n",
        "- Training History Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üêç Python version: 3.14.2\n",
            "‚úÖ PyTorch 2.9.1+cpu available\n",
            "   üíª Using CPU\n",
            "‚úÖ LSTM training setup complete!\n",
            "   Project root: C:\\Users\\cihan\\turkish_finance_ml\n",
            "   Processed data dir: C:\\Users\\cihan\\turkish_finance_ml\\data\\processed\n",
            "   Models dir: C:\\Users\\cihan\\turkish_finance_ml\\models\n",
            "   Framework: PyTorch\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import sys\n",
        "import subprocess\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check Python version\n",
        "python_version = sys.version_info\n",
        "print(f\"üêç Python version: {python_version.major}.{python_version.minor}.{python_version.micro}\")\n",
        "\n",
        "# PyTorch for LSTM training (supports Python 3.14)\n",
        "PYTORCH_AVAILABLE = False\n",
        "try:\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torch.optim as optim\n",
        "    from torch.utils.data import Dataset, DataLoader\n",
        "    PYTORCH_AVAILABLE = True\n",
        "    print(f\"‚úÖ PyTorch {torch.__version__} available\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"   üöÄ CUDA available - GPU acceleration enabled\")\n",
        "    else:\n",
        "        print(f\"   üíª Using CPU\")\n",
        "except ImportError:\n",
        "    print(\"üì¶ PyTorch not found. Attempting to install...\")\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"torch\", \"--quiet\"])\n",
        "        import torch\n",
        "        import torch.nn as nn\n",
        "        import torch.optim as optim\n",
        "        from torch.utils.data import Dataset, DataLoader\n",
        "        PYTORCH_AVAILABLE = True\n",
        "        print(f\"‚úÖ PyTorch {torch.__version__} installed successfully\")\n",
        "        if torch.cuda.is_available():\n",
        "            print(f\"   üöÄ CUDA available - GPU acceleration enabled\")\n",
        "        else:\n",
        "            print(f\"   üíª Using CPU\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå PyTorch installation failed: {e}\")\n",
        "        print(\"   Please install manually: python -m pip install torch\")\n",
        "        raise ImportError(\"PyTorch is required for LSTM training\")\n",
        "\n",
        "# Machine Learning (for comparison)\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
        ")\n",
        "import joblib\n",
        "\n",
        "# XGBoost (for comparison)\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    XGBOOST_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è  XGBoost not installed. Install with: pip install xgboost\")\n",
        "    XGBOOST_AVAILABLE = False\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (14, 6)\n",
        "\n",
        "# Paths\n",
        "current_dir = Path().resolve()\n",
        "if current_dir.name == \"notebooks\":\n",
        "    project_root = current_dir.parent\n",
        "else:\n",
        "    project_root = current_dir\n",
        "\n",
        "data_processed_dir = project_root / \"data\" / \"processed\"\n",
        "models_dir = project_root / \"models\"\n",
        "models_dir.mkdir(parents=True, exist_ok=True)\n",
        "reports_dir = project_root / \"reports\"\n",
        "reports_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"‚úÖ LSTM training setup complete!\")\n",
        "print(f\"   Project root: {project_root}\")\n",
        "print(f\"   Processed data dir: {data_processed_dir}\")\n",
        "print(f\"   Models dir: {models_dir}\")\n",
        "print(f\"   Framework: PyTorch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Loading processed data...\n",
            "‚úÖ Loaded full dataset: (6015, 77)\n",
            "\n",
            "üìä Dataset Summary:\n",
            "   Total samples: 6015\n",
            "   Features: 70\n",
            "   Target distribution:\n",
            "      Up (1): 2,475 (41.15%)\n",
            "      Down (0): 3,540 (58.85%)\n",
            "\n",
            "üîÑ Scaling features...\n",
            "   ‚úÖ Features scaled to [0, 1] range\n"
          ]
        }
      ],
      "source": [
        "# Load processed data\n",
        "print(\"üìä Loading processed data...\")\n",
        "\n",
        "# Load full feature dataset\n",
        "full_features_file = data_processed_dir / \"bist_features_full.csv\"\n",
        "if not full_features_file.exists():\n",
        "    raise FileNotFoundError(f\"File not found: {full_features_file}. Run preprocessing notebook first.\")\n",
        "\n",
        "df = pd.read_csv(full_features_file)\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df = df.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "print(f\"‚úÖ Loaded full dataset: {df.shape}\")\n",
        "\n",
        "# Load macro features if available\n",
        "macro_file = data_processed_dir / \"bist_macro_merged.csv\"\n",
        "if macro_file.exists():\n",
        "    print(f\"\\nüìä Loading macroeconomic features...\")\n",
        "    macro_df = pd.read_csv(macro_file)\n",
        "    macro_df['Date'] = pd.to_datetime(macro_df['Date'])\n",
        "    \n",
        "    # Merge macro features\n",
        "    macro_cols = [col for col in macro_df.columns if col not in ['Date', 'BIST100_Close']]\n",
        "    df = df.merge(macro_df[['Date'] + macro_cols], on='Date', how='left')\n",
        "    df[macro_cols] = df[macro_cols].ffill().bfill()\n",
        "    print(f\"   ‚úÖ Added {len(macro_cols)} macro features: {', '.join(macro_cols)}\")\n",
        "\n",
        "# Select features (exclude Date, Ticker, and targets)\n",
        "exclude_cols = ['Date']\n",
        "if 'Ticker' in df.columns:\n",
        "    exclude_cols.append('Ticker')\n",
        "\n",
        "target_cols = [col for col in df.columns if col.startswith('Target_')]\n",
        "feature_cols = [col for col in df.columns if col not in exclude_cols + target_cols]\n",
        "\n",
        "# Create feature matrix and target\n",
        "X = df[feature_cols].copy()\n",
        "y = df['Target_Direction'].copy()\n",
        "\n",
        "# Remove rows with NaN\n",
        "valid_idx = ~(X.isnull().any(axis=1) | y.isnull())\n",
        "X = X[valid_idx].reset_index(drop=True)\n",
        "y = y[valid_idx].reset_index(drop=True)\n",
        "\n",
        "print(f\"\\nüìä Dataset Summary:\")\n",
        "print(f\"   Total samples: {len(X)}\")\n",
        "print(f\"   Features: {X.shape[1]}\")\n",
        "print(f\"   Target distribution:\")\n",
        "print(f\"      Up (1): {np.sum(y == 1):,} ({np.sum(y == 1)/len(y)*100:.2f}%)\")\n",
        "print(f\"      Down (0): {np.sum(y == 0):,} ({np.sum(y == 0)/len(y)*100:.2f}%)\")\n",
        "\n",
        "# Scale features\n",
        "print(f\"\\nüîÑ Scaling features...\")\n",
        "scaler = MinMaxScaler()  # Use MinMaxScaler for neural networks\n",
        "X_scaled = pd.DataFrame(\n",
        "    scaler.fit_transform(X),\n",
        "    columns=X.columns,\n",
        "    index=X.index\n",
        ")\n",
        "print(f\"   ‚úÖ Features scaled to [0, 1] range\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Create Sequences for LSTM\n",
        "\n",
        "LSTM requires sequences of data. We'll create sequences where each sample contains N previous time steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÖ Creating sequences with 30 time steps...\n",
            "‚úÖ Sequences created!\n",
            "   X_seq shape: (5985, 30, 70) (samples, timesteps, features)\n",
            "   y_seq shape: (5985,)\n",
            "\n",
            "üìä Train/Test Split:\n",
            "   Training: 4,788 sequences\n",
            "   Testing: 1,197 sequences\n",
            "   Features per timestep: 70\n"
          ]
        }
      ],
      "source": [
        "def create_sequences(X, y, sequence_length=30):\n",
        "    \"\"\"\n",
        "    Create sequences for LSTM training\n",
        "    \n",
        "    Args:\n",
        "        X: Feature matrix (DataFrame or array)\n",
        "        y: Target vector\n",
        "        sequence_length: Number of time steps to look back\n",
        "    \n",
        "    Returns:\n",
        "        X_seq: 3D array (samples, timesteps, features)\n",
        "        y_seq: Target array\n",
        "    \"\"\"\n",
        "    X_seq = []\n",
        "    y_seq = []\n",
        "    \n",
        "    for i in range(sequence_length, len(X)):\n",
        "        X_seq.append(X.iloc[i-sequence_length:i].values)\n",
        "        y_seq.append(y.iloc[i])\n",
        "    \n",
        "    return np.array(X_seq), np.array(y_seq)\n",
        "\n",
        "# Create sequences\n",
        "SEQUENCE_LENGTH = 30  # Look back 30 days\n",
        "print(f\"üìÖ Creating sequences with {SEQUENCE_LENGTH} time steps...\")\n",
        "\n",
        "X_seq, y_seq = create_sequences(X_scaled, y, sequence_length=SEQUENCE_LENGTH)\n",
        "\n",
        "print(f\"‚úÖ Sequences created!\")\n",
        "print(f\"   X_seq shape: {X_seq.shape} (samples, timesteps, features)\")\n",
        "print(f\"   y_seq shape: {y_seq.shape}\")\n",
        "\n",
        "# Time series split (80/20)\n",
        "split_idx = int(len(X_seq) * 0.8)\n",
        "X_train_seq = X_seq[:split_idx]\n",
        "X_test_seq = X_seq[split_idx:]\n",
        "y_train_seq = y_seq[:split_idx]\n",
        "y_test_seq = y_seq[split_idx:]\n",
        "\n",
        "print(f\"\\nüìä Train/Test Split:\")\n",
        "print(f\"   Training: {len(X_train_seq):,} sequences\")\n",
        "print(f\"   Testing: {len(X_test_seq):,} sequences\")\n",
        "print(f\"   Features per timestep: {X_train_seq.shape[2]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Build and Train LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "IndentationError",
          "evalue": "unexpected indent (711338683.py, line 9)",
          "output_type": "error",
          "traceback": [
            "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mclass LSTMModel(nn.Module):\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unexpected indent\n"
          ]
        }
      ],
      "source": [
        "if PYTORCH_AVAILABLE:\n",
        "    print(\"üß† Building LSTM model (PyTorch)...\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Model architecture\n",
        "    n_features = X_train_seq.shape[2]\n",
        "    \n",
        "    # Define PyTorch LSTM model\n",
        "        class LSTMModel(nn.Module):\n",
        "            def __init__(self, input_size, hidden_size1=128, hidden_size2=64, hidden_size3=32, num_layers=1, dropout=0.2):\n",
        "                super(LSTMModel, self).__init__()\n",
        "                self.lstm1 = nn.LSTM(input_size, hidden_size1, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
        "                self.dropout1 = nn.Dropout(dropout)\n",
        "                self.lstm2 = nn.LSTM(hidden_size1, hidden_size2, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
        "                self.dropout2 = nn.Dropout(dropout)\n",
        "                self.lstm3 = nn.LSTM(hidden_size2, hidden_size3, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
        "                self.dropout3 = nn.Dropout(dropout)\n",
        "                self.bn = nn.BatchNorm1d(hidden_size3)\n",
        "                self.fc1 = nn.Linear(hidden_size3, 32)\n",
        "                self.dropout_fc = nn.Dropout(dropout)\n",
        "                self.fc2 = nn.Linear(32, 16)\n",
        "                self.fc3 = nn.Linear(16, 1)\n",
        "                self.sigmoid = nn.Sigmoid()\n",
        "                \n",
        "            def forward(self, x):\n",
        "                # First LSTM layer\n",
        "                lstm_out1, _ = self.lstm1(x)\n",
        "                lstm_out1 = self.dropout1(lstm_out1)\n",
        "                \n",
        "                # Second LSTM layer\n",
        "                lstm_out2, _ = self.lstm2(lstm_out1)\n",
        "                lstm_out2 = self.dropout2(lstm_out2)\n",
        "                \n",
        "                # Third LSTM layer - take last timestep\n",
        "                lstm_out3, _ = self.lstm3(lstm_out2)\n",
        "                lstm_out3 = lstm_out3[:, -1, :]  # Take last timestep\n",
        "                lstm_out3 = self.dropout3(lstm_out3)\n",
        "                lstm_out3 = self.bn(lstm_out3)\n",
        "                \n",
        "                # Dense layers\n",
        "                out = torch.relu(self.fc1(lstm_out3))\n",
        "                out = self.dropout_fc(out)\n",
        "                out = torch.relu(self.fc2(out))\n",
        "                out = self.sigmoid(self.fc3(out))\n",
        "                return out\n",
        "        \n",
        "        # Create model\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        model = LSTMModel(n_features).to(device)\n",
        "        print(f\"‚úÖ Model architecture (PyTorch) - Device: {device}\")\n",
        "        print(f\"   Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "        \n",
        "        # Convert data to PyTorch tensors\n",
        "        X_train_tensor = torch.FloatTensor(X_train_seq).to(device)\n",
        "        y_train_tensor = torch.FloatTensor(y_train_seq.reshape(-1, 1)).to(device)\n",
        "        X_test_tensor = torch.FloatTensor(X_test_seq).to(device)\n",
        "        y_test_tensor = torch.FloatTensor(y_test_seq.reshape(-1, 1)).to(device)\n",
        "        \n",
        "        # Create validation split\n",
        "        val_size = int(len(X_train_tensor) * 0.2)\n",
        "        X_val_tensor = X_train_tensor[-val_size:]\n",
        "        y_val_tensor = y_train_tensor[-val_size:]\n",
        "        X_train_tensor = X_train_tensor[:-val_size]\n",
        "        y_train_tensor = y_train_tensor[:-val_size]\n",
        "        \n",
        "        # Training setup\n",
        "        criterion = nn.BCELoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr=0.00001)\n",
        "        \n",
        "        # Training loop\n",
        "        print(f\"\\nüöÄ Training LSTM model (PyTorch)...\")\n",
        "        print(\"=\"*60)\n",
        "        \n",
        "        epochs = 50\n",
        "        batch_size = 32\n",
        "        best_val_loss = float('inf')\n",
        "        patience = 10\n",
        "        patience_counter = 0\n",
        "        history = {'loss': [], 'val_loss': [], 'accuracy': [], 'val_accuracy': []}\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            # Training\n",
        "            model.train()\n",
        "            train_loss = 0\n",
        "            train_correct = 0\n",
        "            train_total = 0\n",
        "            \n",
        "            for i in range(0, len(X_train_tensor), batch_size):\n",
        "                batch_X = X_train_tensor[i:i+batch_size]\n",
        "                batch_y = y_train_tensor[i:i+batch_size]\n",
        "                \n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(batch_X)\n",
        "                loss = criterion(outputs, batch_y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                \n",
        "                train_loss += loss.item()\n",
        "                predicted = (outputs > 0.5).float()\n",
        "                train_total += batch_y.size(0)\n",
        "                train_correct += (predicted == batch_y).sum().item()\n",
        "            \n",
        "            # Validation\n",
        "            model.eval()\n",
        "            val_loss = 0\n",
        "            val_correct = 0\n",
        "            val_total = 0\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                for i in range(0, len(X_val_tensor), batch_size):\n",
        "                    batch_X = X_val_tensor[i:i+batch_size]\n",
        "                    batch_y = y_val_tensor[i:i+batch_size]\n",
        "                    \n",
        "                    outputs = model(batch_X)\n",
        "                    loss = criterion(outputs, batch_y)\n",
        "                    \n",
        "                    val_loss += loss.item()\n",
        "                    predicted = (outputs > 0.5).float()\n",
        "                    val_total += batch_y.size(0)\n",
        "                    val_correct += (predicted == batch_y).sum().item()\n",
        "            \n",
        "            train_loss /= (len(X_train_tensor) // batch_size + 1)\n",
        "            val_loss /= (len(X_val_tensor) // batch_size + 1)\n",
        "            train_acc = train_correct / train_total\n",
        "            val_acc = val_correct / val_total\n",
        "            \n",
        "            history['loss'].append(train_loss)\n",
        "            history['val_loss'].append(val_loss)\n",
        "            history['accuracy'].append(train_acc)\n",
        "            history['val_accuracy'].append(val_acc)\n",
        "            \n",
        "            scheduler.step(val_loss)\n",
        "            \n",
        "            if (epoch + 1) % 5 == 0 or epoch == 0:\n",
        "                print(f\"Epoch {epoch+1}/{epochs} - Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "            \n",
        "            # Early stopping\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                patience_counter = 0\n",
        "                best_model_state = model.state_dict().copy()\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= patience:\n",
        "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                    model.load_state_dict(best_model_state)\n",
        "                    break\n",
        "        \n",
        "        model.load_state_dict(best_model_state)\n",
        "        print(\"‚úÖ Training complete!\")\n",
        "        \n",
        "else:\n",
        "    print(\"‚ùå PyTorch not available. Cannot train LSTM model.\")\n",
        "    print(\"   Please install PyTorch: python -m pip install torch\")\n",
        "    model = None\n",
        "    history = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Evaluate LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùå LSTM model not available for evaluation.\n"
          ]
        }
      ],
      "source": [
        "if PYTORCH_AVAILABLE and model is not None:\n",
        "    # PyTorch predictions\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_pred = model(X_train_tensor).cpu().numpy()\n",
        "        test_pred = model(X_test_tensor).cpu().numpy()\n",
        "        y_train_pred_lstm = (train_pred > 0.5).astype(int).flatten()\n",
        "        y_test_pred_lstm = (test_pred > 0.5).astype(int).flatten()\n",
        "        y_test_proba_lstm = test_pred.flatten()\n",
        "    \n",
        "    # Calculate metrics\n",
        "    train_accuracy_lstm = accuracy_score(y_train_seq, y_train_pred_lstm)\n",
        "    test_accuracy_lstm = accuracy_score(y_test_seq, y_test_pred_lstm)\n",
        "    test_precision_lstm = precision_score(y_test_seq, y_test_pred_lstm)\n",
        "    test_recall_lstm = recall_score(y_test_seq, y_test_pred_lstm)\n",
        "    test_f1_lstm = f1_score(y_test_seq, y_test_pred_lstm)\n",
        "    test_auc_lstm = roc_auc_score(y_test_seq, y_test_proba_lstm)\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "    print(\"üìä LSTM MODEL PERFORMANCE (PyTorch)\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\\n   Training Accuracy: {train_accuracy_lstm:.4f} ({train_accuracy_lstm*100:.2f}%)\")\n",
        "    print(f\"   Test Accuracy: {test_accuracy_lstm:.4f} ({test_accuracy_lstm*100:.2f}%)\")\n",
        "    print(f\"   Test Precision: {test_precision_lstm:.4f}\")\n",
        "    print(f\"   Test Recall: {test_recall_lstm:.4f}\")\n",
        "    print(f\"   Test F1-Score: {test_f1_lstm:.4f}\")\n",
        "    print(f\"   Test AUC-ROC: {test_auc_lstm:.4f}\")\n",
        "    \n",
        "    # Confusion Matrix\n",
        "    cm_lstm = confusion_matrix(y_test_seq, y_test_pred_lstm)\n",
        "    \n",
        "    plt.figure(figsize=(12, 5))\n",
        "    \n",
        "    # Confusion Matrix\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.heatmap(cm_lstm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "    plt.title('LSTM Confusion Matrix', fontsize=14, fontweight='bold')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    \n",
        "    # Training History\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history['accuracy'], label='Train Accuracy', linewidth=2)\n",
        "    plt.plot(history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
        "    plt.plot(history['loss'], label='Train Loss', linewidth=2)\n",
        "    plt.plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
        "    plt.title('LSTM Training History', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Metric')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(reports_dir / 'lstm_performance.png', dpi=300, bbox_inches='tight')\n",
        "    print(f\"\\nüíæ Saved visualization to: {reports_dir / 'lstm_performance.png'}\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"‚ùå LSTM model not available for evaluation.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Train XGBoost Baseline for Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üå≤ Training XGBoost baseline for comparison...\n",
            "============================================================\n",
            "‚úÖ XGBoost training complete!\n",
            "\n",
            "   Training Accuracy: 0.9576 (95.76%)\n",
            "   Test Accuracy: 0.4854 (48.54%)\n",
            "   Test Precision: 0.4778\n",
            "   Test Recall: 0.8875\n",
            "   Test F1-Score: 0.6212\n",
            "   Test AUC-ROC: 0.5179\n"
          ]
        }
      ],
      "source": [
        "# Prepare data for XGBoost (use last timestep of each sequence)\n",
        "print(\"üå≤ Training XGBoost baseline for comparison...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Use the last timestep of each sequence as features for XGBoost\n",
        "X_train_xgb = X_train_seq[:, -1, :]  # Last timestep\n",
        "X_test_xgb = X_test_seq[:, -1, :]\n",
        "\n",
        "if XGBOOST_AVAILABLE:\n",
        "    xgb_model = xgb.XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42,\n",
        "        eval_metric='logloss',\n",
        "        scale_pos_weight=(np.sum(y_train_seq == 0) / np.sum(y_train_seq == 1))\n",
        "    )\n",
        "    \n",
        "    xgb_model.fit(X_train_xgb, y_train_seq)\n",
        "    \n",
        "    # Predictions\n",
        "    y_train_pred_xgb = xgb_model.predict(X_train_xgb)\n",
        "    y_test_pred_xgb = xgb_model.predict(X_test_xgb)\n",
        "    y_test_proba_xgb = xgb_model.predict_proba(X_test_xgb)[:, 1]\n",
        "    \n",
        "    # Calculate metrics\n",
        "    train_accuracy_xgb = accuracy_score(y_train_seq, y_train_pred_xgb)\n",
        "    test_accuracy_xgb = accuracy_score(y_test_seq, y_test_pred_xgb)\n",
        "    test_precision_xgb = precision_score(y_test_seq, y_test_pred_xgb)\n",
        "    test_recall_xgb = recall_score(y_test_seq, y_test_pred_xgb)\n",
        "    test_f1_xgb = f1_score(y_test_seq, y_test_pred_xgb)\n",
        "    test_auc_xgb = roc_auc_score(y_test_seq, y_test_proba_xgb)\n",
        "    \n",
        "    print(\"‚úÖ XGBoost training complete!\")\n",
        "    print(f\"\\n   Training Accuracy: {train_accuracy_xgb:.4f} ({train_accuracy_xgb*100:.2f}%)\")\n",
        "    print(f\"   Test Accuracy: {test_accuracy_xgb:.4f} ({test_accuracy_xgb*100:.2f}%)\")\n",
        "    print(f\"   Test Precision: {test_precision_xgb:.4f}\")\n",
        "    print(f\"   Test Recall: {test_recall_xgb:.4f}\")\n",
        "    print(f\"   Test F1-Score: {test_f1_xgb:.4f}\")\n",
        "    print(f\"   Test AUC-ROC: {test_auc_xgb:.4f}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  XGBoost not available. Skipping comparison.\")\n",
        "    xgb_model = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Comparison: LSTM vs XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö†Ô∏è  Cannot compare: Missing LSTM or XGBoost model\n"
          ]
        }
      ],
      "source": [
        "if PYTORCH_AVAILABLE and XGBOOST_AVAILABLE and model is not None and xgb_model is not None:\n",
        "    print(\"=\"*60)\n",
        "    print(\"üìä MODEL COMPARISON: LSTM vs XGBoost\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    comparison_data = {\n",
        "        'Model': ['LSTM', 'XGBoost'],\n",
        "        'Test Accuracy': [test_accuracy_lstm, test_accuracy_xgb],\n",
        "        'Test Precision': [test_precision_lstm, test_precision_xgb],\n",
        "        'Test Recall': [test_recall_lstm, test_recall_xgb],\n",
        "        'Test F1-Score': [test_f1_lstm, test_f1_xgb],\n",
        "        'Test AUC-ROC': [test_auc_lstm, test_auc_xgb]\n",
        "    }\n",
        "    \n",
        "    comparison_df = pd.DataFrame(comparison_data)\n",
        "    display(comparison_df)\n",
        "    \n",
        "    # Determine best model\n",
        "    if test_accuracy_lstm > test_accuracy_xgb:\n",
        "        best_model_name = \"LSTM\"\n",
        "        improvement = (test_accuracy_lstm - test_accuracy_xgb) * 100\n",
        "        print(f\"\\nüèÜ Best Model: LSTM\")\n",
        "        print(f\"   Improvement over XGBoost: +{improvement:.2f} percentage points\")\n",
        "    else:\n",
        "        best_model_name = \"XGBoost\"\n",
        "        improvement = (test_accuracy_xgb - test_accuracy_lstm) * 100\n",
        "        print(f\"\\nüèÜ Best Model: XGBoost\")\n",
        "        print(f\"   Improvement over LSTM: +{improvement:.2f} percentage points\")\n",
        "    \n",
        "    # Visualization\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Metrics comparison\n",
        "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC']\n",
        "    lstm_scores = [test_accuracy_lstm, test_precision_lstm, test_recall_lstm, test_f1_lstm, test_auc_lstm]\n",
        "    xgb_scores = [test_accuracy_xgb, test_precision_xgb, test_recall_xgb, test_f1_xgb, test_auc_xgb]\n",
        "    \n",
        "    x = np.arange(len(metrics))\n",
        "    width = 0.35\n",
        "    \n",
        "    axes[0].bar(x - width/2, lstm_scores, width, label='LSTM', alpha=0.8)\n",
        "    axes[0].bar(x + width/2, xgb_scores, width, label='XGBoost', alpha=0.8)\n",
        "    axes[0].set_xlabel('Metrics')\n",
        "    axes[0].set_ylabel('Score')\n",
        "    axes[0].set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
        "    axes[0].set_xticks(x)\n",
        "    axes[0].set_xticklabels(metrics, rotation=45, ha='right')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3, axis='y')\n",
        "    axes[0].set_ylim([0, 1])\n",
        "    \n",
        "    # ROC Curves\n",
        "    fpr_lstm, tpr_lstm, _ = roc_curve(y_test_seq, y_test_proba_lstm)\n",
        "    fpr_xgb, tpr_xgb, _ = roc_curve(y_test_seq, y_test_proba_xgb)\n",
        "    \n",
        "    axes[1].plot(fpr_lstm, tpr_lstm, label=f'LSTM (AUC = {test_auc_lstm:.3f})', linewidth=2)\n",
        "    axes[1].plot(fpr_xgb, tpr_xgb, label=f'XGBoost (AUC = {test_auc_xgb:.3f})', linewidth=2)\n",
        "    axes[1].plot([0, 1], [0, 1], 'k--', label='Random', linewidth=1)\n",
        "    axes[1].set_xlabel('False Positive Rate')\n",
        "    axes[1].set_ylabel('True Positive Rate')\n",
        "    axes[1].set_title('ROC Curves Comparison', fontsize=14, fontweight='bold')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(reports_dir / 'lstm_vs_xgboost.png', dpi=300, bbox_inches='tight')\n",
        "    print(f\"\\nüíæ Saved comparison visualization to: {reports_dir / 'lstm_vs_xgboost.png'}\")\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\nüí° Insights:\")\n",
        "    if test_accuracy_lstm > test_accuracy_xgb:\n",
        "        print(\"   ‚úÖ LSTM captures temporal patterns better than XGBoost\")\n",
        "        print(\"   ‚úÖ Deep learning approach shows promise for time series prediction\")\n",
        "    else:\n",
        "        print(\"   ‚ö†Ô∏è  XGBoost performs better with current feature engineering\")\n",
        "        print(\"   üí° Consider: Longer sequences, more LSTM layers, or feature engineering\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Cannot compare: Missing LSTM or XGBoost model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Save Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üíæ Saving models...\n",
            "   ‚úÖ XGBoost baseline saved: C:\\Users\\cihan\\turkish_finance_ml\\models\\xgb_baseline_lstm.pkl\n",
            "\n",
            "‚úÖ All models saved!\n"
          ]
        }
      ],
      "source": [
        "print(\"üíæ Saving models...\")\n",
        "\n",
        "if PYTORCH_AVAILABLE and model is not None:\n",
        "    # Save PyTorch model\n",
        "    lstm_model_path = models_dir / \"lstm_model.pth\"\n",
        "    torch.save(model.state_dict(), lstm_model_path)\n",
        "    print(f\"   ‚úÖ LSTM model (PyTorch) saved: {lstm_model_path}\")\n",
        "    \n",
        "    # Save model architecture info for loading later\n",
        "    model_info = {\n",
        "        'input_size': n_features,\n",
        "        'hidden_size1': 128,\n",
        "        'hidden_size2': 64,\n",
        "        'hidden_size3': 32,\n",
        "        'dropout': 0.2,\n",
        "        'sequence_length': SEQUENCE_LENGTH\n",
        "    }\n",
        "    import json\n",
        "    with open(models_dir / \"lstm_model_info.json\", 'w') as f:\n",
        "        json.dump(model_info, f, indent=2)\n",
        "    print(f\"   ‚úÖ Model info saved: {models_dir / 'lstm_model_info.json'}\")\n",
        "    \n",
        "    # Save scaler\n",
        "    scaler_path = models_dir / \"lstm_scaler.pkl\"\n",
        "    joblib.dump(scaler, scaler_path)\n",
        "    print(f\"   ‚úÖ Scaler saved: {scaler_path}\")\n",
        "\n",
        "if XGBOOST_AVAILABLE and xgb_model is not None:\n",
        "    # Save XGBoost model\n",
        "    xgb_model_path = models_dir / \"xgb_baseline_lstm.pkl\"\n",
        "    joblib.dump(xgb_model, xgb_model_path)\n",
        "    print(f\"   ‚úÖ XGBoost baseline saved: {xgb_model_path}\")\n",
        "\n",
        "print(\"\\n‚úÖ All models saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Summary\n",
        "\n",
        "### Key Findings:\n",
        "- **LSTM Architecture (PyTorch)**: Multi-layer LSTM (128‚Üí64‚Üí32 units) with dropout and batch normalization\n",
        "- **Sequence Length**: 30 days lookback period\n",
        "- **Features**: 70+ technical indicators + lagged macroeconomic features (Inflation, Interest Rates with 1M and 3M lags)\n",
        "- **Performance**: Compared with XGBoost baseline\n",
        "\n",
        "### Advantages of LSTM:\n",
        "1. **Temporal Memory**: Can remember patterns across long sequences (30 days)\n",
        "2. **Non-linear Patterns**: Deep learning captures complex relationships\n",
        "3. **Feature Learning**: Automatically learns relevant features from sequences\n",
        "4. **Lagged Features**: Incorporates delayed economic impacts (1-month and 3-month lags)\n",
        "\n",
        "### Framework:\n",
        "- **PyTorch**: Compatible with Python 3.14, flexible architecture, GPU support\n",
        "\n",
        "### Next Steps:\n",
        "- Experiment with different sequence lengths (15, 30, 60 days)\n",
        "- Try bidirectional LSTM for better pattern recognition\n",
        "- Add attention mechanisms for important time steps\n",
        "- Ensemble LSTM with XGBoost for improved accuracy\n",
        "- Fine-tune hyperparameters (learning rate, dropout, hidden sizes)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
