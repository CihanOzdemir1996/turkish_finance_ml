{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 6: Macroeconomic Data Integration\n",
        "\n",
        "## Goal\n",
        "Integrate macroeconomic indicators (Inflation and Interest Rates) from CBRT EVDS API with BIST-100 stock price data to analyze correlations and relationships.\n",
        "\n",
        "## What This Notebook Does:\n",
        "1. Installs required libraries (`evds` and `python-dotenv`)\n",
        "2. Loads EVDS API key from `.env` file\n",
        "3. Fetches Inflation (TP.FG.J0) and Interest Rates (TP.AP01.TUR) for the last 5 years\n",
        "4. Resamples monthly/weekly data to daily frequency to match BIST-100 data\n",
        "5. Merges macroeconomic data with BIST-100 stock prices\n",
        "6. Analyzes correlation between Inflation and BIST-100 prices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_package(package):\n",
        "    \"\"\"Install a package using pip\"\"\"\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"-q\"])\n",
        "        print(f\"‚úÖ {package} installed successfully\")\n",
        "        return True\n",
        "    except subprocess.CalledProcessError:\n",
        "        print(f\"‚ùå Failed to install {package}\")\n",
        "        return False\n",
        "\n",
        "# Install evds and python-dotenv\n",
        "print(\"üì¶ Installing required packages...\")\n",
        "install_package(\"evds\")\n",
        "install_package(\"python-dotenv\")\n",
        "print(\"\\n‚úÖ All packages installed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Load API Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add src to path for load_env\n",
        "current_dir = Path().resolve()\n",
        "if current_dir.name == \"notebooks\":\n",
        "    project_root = current_dir.parent\n",
        "else:\n",
        "    project_root = current_dir\n",
        "\n",
        "sys.path.append(str(project_root / \"src\"))\n",
        "\n",
        "# Import environment loader\n",
        "from load_env import get_evds_api_key\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load .env file\n",
        "env_file = project_root / \".env\"\n",
        "if env_file.exists():\n",
        "    load_dotenv(env_file)\n",
        "    print(\"‚úÖ Loaded .env file\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  .env file not found, trying environment variables...\")\n",
        "\n",
        "# Get API key\n",
        "EVDS_API_KEY = get_evds_api_key()\n",
        "\n",
        "if EVDS_API_KEY:\n",
        "    print(f\"‚úÖ EVDS API Key loaded: {EVDS_API_KEY[:10]}...{EVDS_API_KEY[-5:]}\")\n",
        "else:\n",
        "    print(\"‚ùå ERROR: EVDS_API_KEY not found!\")\n",
        "    print(\"   Please ensure your .env file contains: EVDS_API_KEY=your_key_here\")\n",
        "    raise ValueError(\"EVDS_API_KEY is required\")\n",
        "\n",
        "# Set up paths\n",
        "data_raw_dir = project_root / \"data\" / \"raw\"\n",
        "data_processed_dir = project_root / \"data\" / \"processed\"\n",
        "reports_dir = project_root / \"reports\"\n",
        "\n",
        "# Create directories if needed\n",
        "data_processed_dir.mkdir(parents=True, exist_ok=True)\n",
        "reports_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Set plotting style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (14, 6)\n",
        "\n",
        "print(f\"\\nüìÇ Project root: {project_root}\")\n",
        "print(f\"üìÇ Raw data dir: {data_raw_dir}\")\n",
        "print(f\"üìÇ Processed data dir: {data_processed_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Fetch Macroeconomic Data from EVDS API\n",
        "\n",
        "We'll fetch:\n",
        "- **TP.FG.J0**: Inflation (T√úFE - Consumer Price Index)\n",
        "- **TP.AP01.TUR**: Interest Rates (Policy Rate)\n",
        "\n",
        "For the last 5 years."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# Calculate date range (last 5 years)\n",
        "end_date = datetime.now()\n",
        "start_date = end_date - timedelta(days=5*365)\n",
        "\n",
        "# Format dates for EVDS API (DD-MM-YYYY)\n",
        "start_date_str = start_date.strftime(\"%d-%m-%Y\")\n",
        "end_date_str = end_date.strftime(\"%d-%m-%Y\")\n",
        "\n",
        "print(f\"üìÖ Date Range: {start_date_str} to {end_date_str}\")\n",
        "print(f\"   (Last 5 years)\\n\")\n",
        "\n",
        "# EVDS API base URL\n",
        "EVDS_BASE_URL = \"https://evds2.tcmb.gov.tr/service/evds\"\n",
        "\n",
        "# Series codes\n",
        "series_codes = {\n",
        "    \"TP.FG.J0\": \"Inflation_TUFE\",  # Consumer Price Index (T√úFE)\n",
        "    \"TP.AP01.TUR\": \"Interest_Rate\"   # Policy Interest Rate\n",
        "}\n",
        "\n",
        "# Fetch data for each series\n",
        "macro_data_list = []\n",
        "\n",
        "for series_code, name in series_codes.items():\n",
        "    print(f\"üìä Fetching {name} (Series: {series_code})...\")\n",
        "    \n",
        "    try:\n",
        "        # EVDS API endpoint\n",
        "        url = f\"{EVDS_BASE_URL}/dataseries/{series_code}\"\n",
        "        params = {\n",
        "            \"key\": EVDS_API_KEY,\n",
        "            \"startDate\": start_date_str,\n",
        "            \"endDate\": end_date_str,\n",
        "            \"type\": \"json\",\n",
        "            \"formulas\": \"0\"  # 0 = raw data\n",
        "        }\n",
        "        \n",
        "        response = requests.get(url, params=params, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        \n",
        "        data = response.json()\n",
        "        \n",
        "        if 'items' in data and len(data['items']) > 0:\n",
        "            # Convert to DataFrame\n",
        "            df = pd.DataFrame(data['items'])\n",
        "            \n",
        "            # Find the date column (usually 'Tarih')\n",
        "            date_col = None\n",
        "            value_col = None\n",
        "            \n",
        "            for col in df.columns:\n",
        "                if 'tarih' in col.lower() or 'date' in col.lower():\n",
        "                    date_col = col\n",
        "                elif col != date_col and df[col].dtype in ['float64', 'int64'] or pd.api.types.is_numeric_dtype(df[col]):\n",
        "                    if value_col is None:\n",
        "                        value_col = col\n",
        "            \n",
        "            if date_col and value_col:\n",
        "                # Clean and format\n",
        "                df_clean = pd.DataFrame({\n",
        "                    'Date': pd.to_datetime(df[date_col], format='%d-%m-%Y', errors='coerce'),\n",
        "                    name: pd.to_numeric(df[value_col], errors='coerce')\n",
        "                })\n",
        "                \n",
        "                # Remove rows with invalid dates or NaN values\n",
        "                df_clean = df_clean.dropna(subset=['Date', name])\n",
        "                df_clean = df_clean.sort_values('Date').reset_index(drop=True)\n",
        "                \n",
        "                macro_data_list.append(df_clean)\n",
        "                print(f\"   ‚úÖ Successfully fetched {len(df_clean)} records\")\n",
        "                print(f\"   üìÖ Date range: {df_clean['Date'].min().date()} to {df_clean['Date'].max().date()}\")\n",
        "            else:\n",
        "                print(f\"   ‚ö†Ô∏è  Could not parse columns. Available columns: {df.columns.tolist()}\")\n",
        "        else:\n",
        "            print(f\"   ‚ö†Ô∏è  No data returned for {series_code}\")\n",
        "        \n",
        "        # Rate limiting\n",
        "        time.sleep(0.5)\n",
        "        \n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        print(f\"   ‚ùå HTTP Error: {e}\")\n",
        "        if response.status_code == 404:\n",
        "            print(f\"   üí° Series code '{series_code}' might be incorrect or not available\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Error: {str(e)}\")\n",
        "    \n",
        "    print()\n",
        "\n",
        "# Merge all macroeconomic series\n",
        "if macro_data_list:\n",
        "    macro_df = macro_data_list[0]\n",
        "    for df in macro_data_list[1:]:\n",
        "        macro_df = macro_df.merge(df, on='Date', how='outer')\n",
        "    \n",
        "    macro_df = macro_df.sort_values('Date').reset_index(drop=True)\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "    print(\"‚úÖ MACROECONOMIC DATA SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\\nüìä Shape: {macro_df.shape}\")\n",
        "    print(f\"üìÖ Date range: {macro_df['Date'].min().date()} to {macro_df['Date'].max().date()}\")\n",
        "    print(f\"üìã Columns: {macro_df.columns.tolist()}\")\n",
        "    print(f\"\\nüìà First few rows:\")\n",
        "    display(macro_df.head(10))\n",
        "    print(f\"\\nüìâ Last few rows:\")\n",
        "    display(macro_df.tail(10))\n",
        "    print(f\"\\nüìä Statistical Summary:\")\n",
        "    display(macro_df.describe())\n",
        "else:\n",
        "    print(\"‚ùå No macroeconomic data was successfully fetched!\")\n",
        "    macro_df = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load BIST stock data\n",
        "stock_file = data_raw_dir / \"bist_stock_prices.csv\"\n",
        "\n",
        "if stock_file.exists():\n",
        "    print(f\"üìä Loading BIST stock data from: {stock_file}\")\n",
        "    bist_df = pd.read_csv(stock_file)\n",
        "    bist_df['Date'] = pd.to_datetime(bist_df['Date'])\n",
        "    bist_df = bist_df.sort_values('Date').reset_index(drop=True)\n",
        "    \n",
        "    # Filter for BIST-100 index if multiple tickers exist\n",
        "    if 'Ticker' in bist_df.columns:\n",
        "        unique_tickers = bist_df['Ticker'].unique()\n",
        "        if 'XU100.IS' in unique_tickers:\n",
        "            bist_df = bist_df[bist_df['Ticker'] == 'XU100.IS'].copy().reset_index(drop=True)\n",
        "            print(f\"‚úÖ Loaded BIST-100 index data (XU100.IS)\")\n",
        "        else:\n",
        "            # Use ticker with most data\n",
        "            ticker_counts = bist_df['Ticker'].value_counts()\n",
        "            selected_ticker = ticker_counts.index[0]\n",
        "            bist_df = bist_df[bist_df['Ticker'] == selected_ticker].copy().reset_index(drop=True)\n",
        "            print(f\"‚ö†Ô∏è  Using ticker: {selected_ticker} (BIST-100 not found)\")\n",
        "    \n",
        "    # Select relevant columns (Date and Close price)\n",
        "    if 'Close' in bist_df.columns:\n",
        "        bist_df = bist_df[['Date', 'Close']].copy()\n",
        "        bist_df = bist_df.rename(columns={'Close': 'BIST100_Close'})\n",
        "    else:\n",
        "        print(\"‚ùå 'Close' column not found in stock data\")\n",
        "        bist_df = pd.DataFrame()\n",
        "    \n",
        "    print(f\"\\nüìä BIST Data Summary:\")\n",
        "    print(f\"   Shape: {bist_df.shape}\")\n",
        "    print(f\"   Date range: {bist_df['Date'].min().date()} to {bist_df['Date'].max().date()}\")\n",
        "    print(f\"\\nüìà First few rows:\")\n",
        "    display(bist_df.head(10))\n",
        "    \n",
        "else:\n",
        "    print(f\"‚ùå Error: {stock_file} not found!\")\n",
        "    print(\"   Please run 01_data_collection.ipynb first to download the data.\")\n",
        "    bist_df = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Resample Macroeconomic Data to Daily Frequency\n",
        "\n",
        "Macroeconomic data from EVDS is typically monthly or weekly. We need to resample it to daily frequency to match the BIST-100 daily stock prices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not macro_df.empty and not bist_df.empty:\n",
        "    print(\"üîÑ Resampling macroeconomic data to daily frequency...\")\n",
        "    \n",
        "    # Set Date as index for resampling\n",
        "    macro_df_indexed = macro_df.set_index('Date').copy()\n",
        "    \n",
        "    # Determine the date range from BIST data\n",
        "    min_date = min(bist_df['Date'].min(), macro_df['Date'].min())\n",
        "    max_date = max(bist_df['Date'].max(), macro_df['Date'].max())\n",
        "    \n",
        "    # Create daily date range\n",
        "    daily_dates = pd.date_range(start=min_date, end=max_date, freq='D')\n",
        "    \n",
        "    # Reindex to daily frequency and forward fill (carry last known value forward)\n",
        "    macro_daily = macro_df_indexed.reindex(daily_dates).ffill()\n",
        "    \n",
        "    # Reset index to get Date as column\n",
        "    macro_daily = macro_daily.reset_index()\n",
        "    macro_daily = macro_daily.rename(columns={'index': 'Date'})\n",
        "    \n",
        "    print(f\"‚úÖ Resampled to daily frequency\")\n",
        "    print(f\"   Original records: {len(macro_df)}\")\n",
        "    print(f\"   Daily records: {len(macro_daily)}\")\n",
        "    print(f\"   Date range: {macro_daily['Date'].min().date()} to {macro_daily['Date'].max().date()}\")\n",
        "    \n",
        "    # Show sample of resampled data\n",
        "    print(f\"\\nüìä Sample of resampled data (showing where values change):\")\n",
        "    # Find rows where values actually change (not just forward-filled)\n",
        "    for col in macro_daily.columns:\n",
        "        if col != 'Date':\n",
        "            # Mark where original data points are\n",
        "            original_dates = set(macro_df['Date'].dt.date)\n",
        "            macro_daily[f'{col}_is_original'] = macro_daily['Date'].dt.date.isin(original_dates)\n",
        "    \n",
        "    # Show a sample\n",
        "    sample_indices = []\n",
        "    for i in range(len(macro_daily)):\n",
        "        if any(macro_daily.iloc[i][col] for col in macro_daily.columns if col.endswith('_is_original')):\n",
        "            sample_indices.append(i)\n",
        "        if len(sample_indices) >= 10:\n",
        "            break\n",
        "    \n",
        "    if sample_indices:\n",
        "        display(macro_daily.iloc[sample_indices][[col for col in macro_daily.columns if not col.endswith('_is_original')]].head(10))\n",
        "    \n",
        "    # Remove helper columns\n",
        "    macro_daily = macro_daily[[col for col in macro_daily.columns if not col.endswith('_is_original')]]\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå Cannot resample: Missing macroeconomic or BIST data\")\n",
        "    macro_daily = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Merge Macroeconomic Data with BIST-100 Prices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not macro_daily.empty and not bist_df.empty:\n",
        "    print(\"üîó Merging macroeconomic data with BIST-100 prices...\")\n",
        "    \n",
        "    # Ensure Date columns are exactly datetime64[ns] type\n",
        "    print(\"\\nüîç Checking date column types...\")\n",
        "    print(f\"   BIST Date type: {bist_df['Date'].dtype}\")\n",
        "    print(f\"   Macro Date type: {macro_daily['Date'].dtype}\")\n",
        "    \n",
        "    # Convert to datetime64[ns] if needed\n",
        "    bist_df['Date'] = pd.to_datetime(bist_df['Date']).astype('datetime64[ns]')\n",
        "    macro_daily['Date'] = pd.to_datetime(macro_daily['Date']).astype('datetime64[ns]')\n",
        "    \n",
        "    print(f\"   After conversion - BIST Date type: {bist_df['Date'].dtype}\")\n",
        "    print(f\"   After conversion - Macro Date type: {macro_daily['Date'].dtype}\")\n",
        "    \n",
        "    # Merge on Date\n",
        "    merged_df = bist_df.merge(macro_daily, on='Date', how='inner')\n",
        "    \n",
        "    # Sort by date\n",
        "    merged_df = merged_df.sort_values('Date').reset_index(drop=True)\n",
        "    \n",
        "    print(f\"\\nüìä After merge (before handling missing values):\")\n",
        "    print(f\"   Shape: {merged_df.shape}\")\n",
        "    print(f\"   Missing values per column:\")\n",
        "    missing_counts = merged_df.isnull().sum()\n",
        "    for col, count in missing_counts.items():\n",
        "        if count > 0:\n",
        "            print(f\"      {col}: {count} ({count/len(merged_df)*100:.2f}%)\")\n",
        "    \n",
        "    # Handle missing values: forward fill then backward fill\n",
        "    print(f\"\\nüîÑ Handling missing values with forward fill and backward fill...\")\n",
        "    merged_df_clean = merged_df.copy()\n",
        "    \n",
        "    # Forward fill (carry last known value forward)\n",
        "    merged_df_clean = merged_df_clean.ffill()\n",
        "    \n",
        "    # Backward fill (fill remaining NaN at the beginning)\n",
        "    merged_df_clean = merged_df_clean.bfill()\n",
        "    \n",
        "    # Check if there are still any NaN values\n",
        "    remaining_nans = merged_df_clean.isnull().sum().sum()\n",
        "    if remaining_nans > 0:\n",
        "        print(f\"   ‚ö†Ô∏è  Still {remaining_nans} NaN values after ffill/bfill. Dropping rows with NaN...\")\n",
        "        merged_df_clean = merged_df_clean.dropna()\n",
        "    else:\n",
        "        print(f\"   ‚úÖ All missing values handled successfully!\")\n",
        "    \n",
        "    # Add lagged macroeconomic features (1-month and 3-month lags)\n",
        "    print(f\"\\nüìÖ Creating lagged macroeconomic features...\")\n",
        "    if 'Inflation_TUFE' in merged_df_clean.columns:\n",
        "        # 1-month lag (approximately 30 days)\n",
        "        merged_df_clean['Inflation_TUFE_Lag_1M'] = merged_df_clean['Inflation_TUFE'].shift(30)\n",
        "        # 3-month lag (approximately 90 days)\n",
        "        merged_df_clean['Inflation_TUFE_Lag_3M'] = merged_df_clean['Inflation_TUFE'].shift(90)\n",
        "        print(f\"   ‚úÖ Added Inflation lags: 1-month, 3-month\")\n",
        "    \n",
        "    if 'Interest_Rate' in merged_df_clean.columns:\n",
        "        # 1-month lag (approximately 30 days)\n",
        "        merged_df_clean['Interest_Rate_Lag_1M'] = merged_df_clean['Interest_Rate'].shift(30)\n",
        "        # 3-month lag (approximately 90 days)\n",
        "        merged_df_clean['Interest_Rate_Lag_3M'] = merged_df_clean['Interest_Rate'].shift(90)\n",
        "        print(f\"   ‚úÖ Added Interest Rate lags: 1-month, 3-month\")\n",
        "    \n",
        "    # Forward fill lagged features\n",
        "    lag_cols = [col for col in merged_df_clean.columns if 'Lag' in col]\n",
        "    if lag_cols:\n",
        "        merged_df_clean[lag_cols] = merged_df_clean[lag_cols].ffill().bfill()\n",
        "        print(f\"   ‚úÖ Filled missing values in lagged features\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ Merge complete!\")\n",
        "    print(f\"   Merged records (before cleaning): {len(merged_df)}\")\n",
        "    print(f\"   Clean records (after ffill/bfill): {len(merged_df_clean)}\")\n",
        "    \n",
        "    if not merged_df_clean.empty:\n",
        "        print(f\"   Date range: {merged_df_clean['Date'].min().date()} to {merged_df_clean['Date'].max().date()}\")\n",
        "        print(f\"   Date column type: {merged_df_clean['Date'].dtype}\")\n",
        "        \n",
        "        print(f\"\\nüìä Merged Dataset Summary:\")\n",
        "        print(f\"   Shape: {merged_df_clean.shape}\")\n",
        "        print(f\"   Columns: {merged_df_clean.columns.tolist()}\")\n",
        "        \n",
        "        print(f\"\\nüìà First 5 rows (VERIFICATION):\")\n",
        "        display(merged_df_clean.head(5))\n",
        "        \n",
        "        print(f\"\\nüìâ Last 5 rows:\")\n",
        "        display(merged_df_clean.tail(5))\n",
        "        \n",
        "        print(f\"\\nüìä Statistical Summary:\")\n",
        "        display(merged_df_clean.describe())\n",
        "        \n",
        "        # Save merged dataset\n",
        "        output_file = data_processed_dir / \"bist_macro_merged.csv\"\n",
        "        merged_df_clean.to_csv(output_file, index=False)\n",
        "        print(f\"\\nüíæ Saved merged dataset to: {output_file}\")\n",
        "    else:\n",
        "        print(\"   ‚ö†Ô∏è  WARNING: Merged dataframe is empty after cleaning!\")\n",
        "        merged_df_clean = pd.DataFrame()\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå Cannot merge: Missing data\")\n",
        "    if macro_daily.empty:\n",
        "        print(\"   - macro_daily is empty\")\n",
        "    if bist_df.empty:\n",
        "        print(\"   - bist_df is empty\")\n",
        "    merged_df_clean = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Correlation Analysis: Inflation vs BIST-100\n",
        "\n",
        "Let's analyze the correlation between Inflation and BIST-100 stock prices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not merged_df_clean.empty:\n",
        "    # Calculate correlation matrix\n",
        "    numeric_cols = merged_df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    \n",
        "    if len(numeric_cols) > 1:\n",
        "        correlation_matrix = merged_df_clean[numeric_cols].corr()\n",
        "        \n",
        "        print(\"=\"*60)\n",
        "        print(\"üìä CORRELATION MATRIX\")\n",
        "        print(\"=\"*60)\n",
        "        display(correlation_matrix)\n",
        "        \n",
        "        # Focus on Inflation vs BIST-100 correlation\n",
        "        if 'Inflation_TUFE' in numeric_cols and 'BIST100_Close' in numeric_cols:\n",
        "            inflation_bist_corr = correlation_matrix.loc['Inflation_TUFE', 'BIST100_Close']\n",
        "            print(f\"\\nüîç Key Finding:\")\n",
        "            print(f\"   Correlation between Inflation (T√úFE) and BIST-100: {inflation_bist_corr:.4f}\")\n",
        "            \n",
        "            if abs(inflation_bist_corr) < 0.1:\n",
        "                interpretation = \"Very weak correlation\"\n",
        "            elif abs(inflation_bist_corr) < 0.3:\n",
        "                interpretation = \"Weak correlation\"\n",
        "            elif abs(inflation_bist_corr) < 0.5:\n",
        "                interpretation = \"Moderate correlation\"\n",
        "            elif abs(inflation_bist_corr) < 0.7:\n",
        "                interpretation = \"Strong correlation\"\n",
        "            else:\n",
        "                interpretation = \"Very strong correlation\"\n",
        "            \n",
        "            direction = \"positive\" if inflation_bist_corr > 0 else \"negative\"\n",
        "            print(f\"   Interpretation: {interpretation} ({direction})\")\n",
        "            \n",
        "            if inflation_bist_corr > 0:\n",
        "                print(f\"   üí° Higher inflation tends to be associated with higher BIST-100 prices\")\n",
        "            else:\n",
        "                print(f\"   üí° Higher inflation tends to be associated with lower BIST-100 prices\")\n",
        "        \n",
        "        # Check Interest Rate correlation if available\n",
        "        if 'Interest_Rate' in numeric_cols and 'BIST100_Close' in numeric_cols:\n",
        "            interest_bist_corr = correlation_matrix.loc['Interest_Rate', 'BIST100_Close']\n",
        "            print(f\"\\n   Correlation between Interest Rate and BIST-100: {interest_bist_corr:.4f}\")\n",
        "            \n",
        "            if interest_bist_corr > 0:\n",
        "                print(f\"   üí° Higher interest rates tend to be associated with higher BIST-100 prices\")\n",
        "            else:\n",
        "                print(f\"   üí° Higher interest rates tend to be associated with lower BIST-100 prices\")\n",
        "        \n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  Not enough numeric columns for correlation analysis\")\n",
        "else:\n",
        "    print(\"‚ùå No merged data available for correlation analysis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify data exists before visualization\n",
        "print(\"=\"*60)\n",
        "print(\"üîç PRE-VISUALIZATION DATA CHECK\")\n",
        "print(\"=\"*60)\n",
        "print(f\"merged_df_clean is empty: {merged_df_clean.empty}\")\n",
        "if not merged_df_clean.empty:\n",
        "    print(f\"Shape: {merged_df_clean.shape}\")\n",
        "    print(f\"Columns: {merged_df_clean.columns.tolist()}\")\n",
        "    print(f\"\\nüìã First 5 rows of merged_df_clean:\")\n",
        "    display(merged_df_clean.head(5))\n",
        "    print(f\"\\nüìä Data types:\")\n",
        "    print(merged_df_clean.dtypes)\n",
        "    print(f\"\\nüîç Missing values check:\")\n",
        "    print(merged_df_clean.isnull().sum())\n",
        "    print(\"=\"*60)\n",
        "\n",
        "if not merged_df_clean.empty:\n",
        "    # Recalculate correlation matrix for visualization\n",
        "    numeric_cols = merged_df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    \n",
        "    print(f\"\\nüìä Numeric columns found: {numeric_cols}\")\n",
        "    \n",
        "    if len(numeric_cols) > 1:\n",
        "        correlation_matrix = merged_df_clean[numeric_cols].corr()\n",
        "        \n",
        "        # Create visualizations\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "        \n",
        "        # 1. Time series: Inflation and BIST-100\n",
        "        ax1 = axes[0, 0]\n",
        "        if 'Inflation_TUFE' in merged_df_clean.columns:\n",
        "            ax1_twin = ax1.twinx()\n",
        "            ax1.plot(merged_df_clean['Date'], merged_df_clean['BIST100_Close'], \n",
        "                    color='blue', label='BIST-100 Close', linewidth=1.5)\n",
        "            ax1_twin.plot(merged_df_clean['Date'], merged_df_clean['Inflation_TUFE'], \n",
        "                         color='red', label='Inflation (T√úFE)', linewidth=1.5, alpha=0.7)\n",
        "            ax1.set_xlabel('Date', fontsize=12)\n",
        "            ax1.set_ylabel('BIST-100 Close Price', fontsize=12, color='blue')\n",
        "            ax1_twin.set_ylabel('Inflation (T√úFE)', fontsize=12, color='red')\n",
        "            ax1.set_title('BIST-100 vs Inflation Over Time', fontsize=14, fontweight='bold')\n",
        "            ax1.tick_params(axis='y', labelcolor='blue')\n",
        "            ax1_twin.tick_params(axis='y', labelcolor='red')\n",
        "            ax1.grid(True, alpha=0.3)\n",
        "            ax1.legend(loc='upper left')\n",
        "            ax1_twin.legend(loc='upper right')\n",
        "        \n",
        "        # 2. Scatter plot: Inflation vs BIST-100\n",
        "        ax2 = axes[0, 1]\n",
        "        if 'Inflation_TUFE' in merged_df_clean.columns and 'BIST100_Close' in numeric_cols:\n",
        "            ax2.scatter(merged_df_clean['Inflation_TUFE'], merged_df_clean['BIST100_Close'], \n",
        "                       alpha=0.5, s=20)\n",
        "            ax2.set_xlabel('Inflation (T√úFE)', fontsize=12)\n",
        "            ax2.set_ylabel('BIST-100 Close Price', fontsize=12)\n",
        "            ax2.set_title('Inflation vs BIST-100 Scatter Plot', fontsize=14, fontweight='bold')\n",
        "            ax2.grid(True, alpha=0.3)\n",
        "            \n",
        "            # Add correlation coefficient\n",
        "            if 'Inflation_TUFE' in numeric_cols:\n",
        "                corr = correlation_matrix.loc['Inflation_TUFE', 'BIST100_Close']\n",
        "                ax2.text(0.05, 0.95, f'Correlation: {corr:.3f}', \n",
        "                        transform=ax2.transAxes, fontsize=11,\n",
        "                        verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "        \n",
        "        # 3. Time series: Interest Rate and BIST-100\n",
        "        ax3 = axes[1, 0]\n",
        "        if 'Interest_Rate' in merged_df_clean.columns:\n",
        "            ax3_twin = ax3.twinx()\n",
        "            ax3.plot(merged_df_clean['Date'], merged_df_clean['BIST100_Close'], \n",
        "                    color='blue', label='BIST-100 Close', linewidth=1.5)\n",
        "            ax3_twin.plot(merged_df_clean['Date'], merged_df_clean['Interest_Rate'], \n",
        "                         color='green', label='Interest Rate', linewidth=1.5, alpha=0.7)\n",
        "            ax3.set_xlabel('Date', fontsize=12)\n",
        "            ax3.set_ylabel('BIST-100 Close Price', fontsize=12, color='blue')\n",
        "            ax3_twin.set_ylabel('Interest Rate (%)', fontsize=12, color='green')\n",
        "            ax3.set_title('BIST-100 vs Interest Rate Over Time', fontsize=14, fontweight='bold')\n",
        "            ax3.tick_params(axis='y', labelcolor='blue')\n",
        "            ax3_twin.tick_params(axis='y', labelcolor='green')\n",
        "            ax3.grid(True, alpha=0.3)\n",
        "            ax3.legend(loc='upper left')\n",
        "            ax3_twin.legend(loc='upper right')\n",
        "        \n",
        "        # 4. Correlation heatmap\n",
        "        ax4 = axes[1, 1]\n",
        "        sns.heatmap(correlation_matrix, annot=True, fmt='.3f', cmap='coolwarm', \n",
        "                   center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8}, ax=ax4)\n",
        "        ax4.set_title('Correlation Heatmap', fontsize=14, fontweight='bold')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        \n",
        "        # Save figure\n",
        "        output_fig = reports_dir / 'macro_bist_correlation.png'\n",
        "        plt.savefig(output_fig, dpi=300, bbox_inches='tight')\n",
        "        print(f\"\\nüíæ Saved correlation visualization to: {output_fig}\")\n",
        "        \n",
        "        plt.show()\n",
        "    else:\n",
        "        print(f\"\\n‚ùå Not enough numeric columns for visualization. Found: {len(numeric_cols)}\")\n",
        "        print(f\"   Required: At least 2 numeric columns\")\n",
        "        print(f\"   Available columns: {merged_df_clean.columns.tolist()}\")\n",
        "else:\n",
        "    print(\"\\n‚ùå No data available for visualization\")\n",
        "    print(\"   merged_df_clean is empty. Please check the merge step above.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Summary\n",
        "\n",
        "This notebook successfully:\n",
        "- ‚úÖ Fetched Inflation (TP.FG.J0) and Interest Rates (TP.AP01.TUR) from CBRT EVDS API\n",
        "- ‚úÖ Resampled monthly/weekly macroeconomic data to daily frequency\n",
        "- ‚úÖ Merged macroeconomic indicators with BIST-100 stock prices\n",
        "- ‚úÖ Created lagged features (1-month and 3-month lags) for Inflation and Interest Rates\n",
        "- ‚úÖ Analyzed correlations between Inflation and BIST-100 prices\n",
        "\n",
        "### Key Features Created:\n",
        "- **Current Features**: Inflation_TUFE, Interest_Rate\n",
        "- **Lagged Features**: \n",
        "  - Inflation_TUFE_Lag_1M (1-month lag)\n",
        "  - Inflation_TUFE_Lag_3M (3-month lag)\n",
        "  - Interest_Rate_Lag_1M (1-month lag)\n",
        "  - Interest_Rate_Lag_3M (3-month lag)\n",
        "\n",
        "### Key Insights:\n",
        "- The correlation analysis reveals the relationship between macroeconomic factors and stock market performance\n",
        "- Lagged features capture delayed effects (e.g., how inflation from last month affects current stock prices)\n",
        "- This merged dataset can be used to enhance machine learning models by including macroeconomic features\n",
        "\n",
        "### Next Steps:\n",
        "- Use the merged dataset (`bist_macro_merged.csv`) in model training to include macroeconomic features\n",
        "- The lagged features help capture temporal relationships between macro indicators and stock prices\n",
        "- Add more macroeconomic indicators (exchange rates, GDP, etc.) for comprehensive analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
