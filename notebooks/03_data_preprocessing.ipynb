{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 3: Data Preprocessing & Feature Engineering\n",
        "\n",
        "## Goal\n",
        "Prepare BIST stock data for machine learning models by:\n",
        "- Cleaning and handling missing values\n",
        "- Engineering technical indicators and features\n",
        "- Creating target variables for prediction\n",
        "- Scaling and normalizing features\n",
        "- Splitting data for training/validation\n",
        "\n",
        "## Focus: BIST-100 Stock Analysis\n",
        "We'll work with the available stock data to create features for trend prediction, volatility forecasting, and price movement classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Preprocessing setup complete!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import os\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "# Paths - Handle both notebook directory and project root\n",
        "# If running from notebooks/, go up one level; if from project root, use current\n",
        "current_dir = Path().resolve()\n",
        "if current_dir.name == \"notebooks\":\n",
        "    project_root = current_dir.parent\n",
        "else:\n",
        "    project_root = current_dir\n",
        "\n",
        "data_raw_dir = project_root / \"data\" / \"raw\"\n",
        "data_processed_dir = project_root / \"data\" / \"processed\"\n",
        "reports_dir = project_root / \"reports\"\n",
        "\n",
        "# Explicitly create directories using os.makedirs with absolute paths\n",
        "data_processed_dir_abs = str(data_processed_dir.resolve())\n",
        "reports_dir_abs = str(reports_dir.resolve())\n",
        "os.makedirs(data_processed_dir_abs, exist_ok=True)\n",
        "os.makedirs(reports_dir_abs, exist_ok=True)\n",
        "\n",
        "print(\"‚úÖ Preprocessing setup complete!\")\n",
        "print(f\"   Project root: {project_root}\")\n",
        "print(f\"   Raw data dir: {data_raw_dir}\")\n",
        "print(f\"   Processed data dir (absolute): {data_processed_dir_abs}\")\n",
        "print(f\"   Raw data exists: {data_raw_dir.exists()}\")\n",
        "print(f\"   Stock file exists: {(data_raw_dir / 'bist_stock_prices.csv').exists()}\")\n",
        "print(f\"   Processed dir created: {os.path.exists(data_processed_dir_abs)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Stock Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Loaded data for AKBNK.IS\n",
            "   Dataset shape: (6609, 9)\n",
            "   Date range: 2000-05-10 to 2026-01-16\n",
            "   Columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits', 'Ticker']\n"
          ]
        }
      ],
      "source": [
        "# Load BIST stock data\n",
        "stock_file = data_raw_dir / \"bist_stock_prices.csv\"\n",
        "\n",
        "print(f\"üìÇ Looking for file: {stock_file}\")\n",
        "print(f\"   File exists: {stock_file.exists()}\")\n",
        "\n",
        "if stock_file.exists():\n",
        "    print(f\"üìä Loading data from: {stock_file}\")\n",
        "    df = pd.read_csv(stock_file)\n",
        "    df['Date'] = pd.to_datetime(df['Date'])\n",
        "    df = df.sort_values('Date').reset_index(drop=True)\n",
        "    \n",
        "    print(f\"\\n   Initial dataset shape: {df.shape}\")\n",
        "    if 'Ticker' in df.columns:\n",
        "        print(f\"   Available tickers: {df['Ticker'].unique()}\")\n",
        "    \n",
        "    # Filter for BIST-100 index if multiple tickers exist\n",
        "    if 'Ticker' in df.columns:\n",
        "        unique_tickers = df['Ticker'].unique()\n",
        "        if 'XU100.IS' in unique_tickers:\n",
        "            df = df[df['Ticker'] == 'XU100.IS'].copy().reset_index(drop=True)\n",
        "            print(f\"‚úÖ Loaded BIST-100 index data (XU100.IS)\")\n",
        "        else:\n",
        "            # Use first ticker if BIST-100 not available\n",
        "            # Count records per ticker and use the one with most data\n",
        "            ticker_counts = df['Ticker'].value_counts()\n",
        "            selected_ticker = ticker_counts.index[0]\n",
        "            df = df[df['Ticker'] == selected_ticker].copy().reset_index(drop=True)\n",
        "            print(f\"‚ö†Ô∏è  BIST-100 (XU100.IS) not found in data.\")\n",
        "            print(f\"   Available tickers: {', '.join(unique_tickers[:5])}...\")\n",
        "            print(f\"   Using ticker with most data: {selected_ticker} ({ticker_counts[selected_ticker]} records)\")\n",
        "    else:\n",
        "        print(\"‚úÖ Loaded data (no ticker column - single stock/index)\")\n",
        "    \n",
        "    print(f\"\\n   Final dataset shape: {df.shape}\")\n",
        "    print(f\"   Date range: {df['Date'].min().date()} to {df['Date'].max().date()}\")\n",
        "    print(f\"   Columns: {df.columns.tolist()}\")\n",
        "else:\n",
        "    print(\"‚ùå Error: bist_stock_prices.csv not found!\")\n",
        "    print(f\"   Expected location: {stock_file}\")\n",
        "    print(f\"   Current working directory: {Path.cwd()}\")\n",
        "    print(f\"   Project root: {project_root}\")\n",
        "    print(f\"   Raw data directory: {data_raw_dir}\")\n",
        "    print(f\"   Please ensure the file exists at the expected location.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Cleaning & Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "MISSING VALUES CHECK\n",
            "============================================================\n",
            "\n",
            "‚úÖ No missing values found!\n"
          ]
        }
      ],
      "source": [
        "# Check for missing values\n",
        "print(\"=\"*60)\n",
        "print(\"MISSING VALUES CHECK\")\n",
        "print(\"=\"*60)\n",
        "missing_count = df.isnull().sum()\n",
        "missing_pct = (missing_count / len(df)) * 100\n",
        "missing_df = pd.DataFrame({\n",
        "    'Column': missing_count.index,\n",
        "    'Missing Count': missing_count.values,\n",
        "    'Missing %': missing_pct.values\n",
        "})\n",
        "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
        "\n",
        "if len(missing_df) > 0:\n",
        "    print(\"\\n‚ö†Ô∏è  Missing values found:\")\n",
        "    display(missing_df)\n",
        "    \n",
        "    # Forward fill for price data, backward fill for volume\n",
        "    price_cols = ['Open', 'High', 'Low', 'Close']\n",
        "    for col in price_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].fillna(method='ffill').fillna(method='bfill')\n",
        "    \n",
        "    if 'Volume' in df.columns:\n",
        "        df['Volume'] = df['Volume'].fillna(method='bfill').fillna(method='ffill')\n",
        "    \n",
        "    print(\"\\n‚úÖ Missing values handled (forward/backward fill)\")\n",
        "else:\n",
        "    print(\"\\n‚úÖ No missing values found!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Feature Engineering - Technical Indicators\n",
        "\n",
        "Creating comprehensive technical indicators for BIST-100 analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Calculating technical indicators...\n",
            "‚úÖ Technical indicators calculated!\n",
            "   Total features: 72\n",
            "   New features added: 63\n"
          ]
        }
      ],
      "source": [
        "# Calculate comprehensive technical indicators\n",
        "df_features = df.copy()\n",
        "df_features = df_features.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "print(\"üìä Calculating technical indicators...\")\n",
        "\n",
        "# === MOVING AVERAGES ===\n",
        "df_features['SMA_5'] = df_features['Close'].rolling(window=5).mean()\n",
        "df_features['SMA_10'] = df_features['Close'].rolling(window=10).mean()\n",
        "df_features['SMA_20'] = df_features['Close'].rolling(window=20).mean()\n",
        "df_features['SMA_50'] = df_features['Close'].rolling(window=50).mean()\n",
        "df_features['SMA_200'] = df_features['Close'].rolling(window=200).mean()\n",
        "\n",
        "# Exponential Moving Averages\n",
        "df_features['EMA_12'] = df_features['Close'].ewm(span=12, adjust=False).mean()\n",
        "df_features['EMA_26'] = df_features['Close'].ewm(span=26, adjust=False).mean()\n",
        "df_features['EMA_50'] = df_features['Close'].ewm(span=50, adjust=False).mean()\n",
        "\n",
        "# Moving Average Crossovers\n",
        "df_features['SMA_Cross_5_20'] = df_features['SMA_5'] - df_features['SMA_20']\n",
        "df_features['SMA_Cross_20_50'] = df_features['SMA_20'] - df_features['SMA_50']\n",
        "df_features['EMA_Cross'] = df_features['EMA_12'] - df_features['EMA_26']\n",
        "\n",
        "# === MACD ===\n",
        "df_features['MACD'] = df_features['EMA_12'] - df_features['EMA_26']\n",
        "df_features['MACD_Signal'] = df_features['MACD'].ewm(span=9, adjust=False).mean()\n",
        "df_features['MACD_Histogram'] = df_features['MACD'] - df_features['MACD_Signal']\n",
        "\n",
        "# === RSI ===\n",
        "def calculate_rsi(prices, period=14):\n",
        "    delta = prices.diff()\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
        "    rs = gain / loss\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "    return rsi\n",
        "\n",
        "df_features['RSI'] = calculate_rsi(df_features['Close'], period=14)\n",
        "df_features['RSI_Overbought'] = (df_features['RSI'] > 70).astype(int)\n",
        "df_features['RSI_Oversold'] = (df_features['RSI'] < 30).astype(int)\n",
        "\n",
        "# === BOLLINGER BANDS ===\n",
        "df_features['BB_Middle'] = df_features['Close'].rolling(window=20).mean()\n",
        "bb_std = df_features['Close'].rolling(window=20).std()\n",
        "df_features['BB_Upper'] = df_features['BB_Middle'] + (bb_std * 2)\n",
        "df_features['BB_Lower'] = df_features['BB_Middle'] - (bb_std * 2)\n",
        "df_features['BB_Width'] = df_features['BB_Upper'] - df_features['BB_Lower']\n",
        "df_features['BB_Position'] = (df_features['Close'] - df_features['BB_Lower']) / (df_features['BB_Upper'] - df_features['BB_Lower'])\n",
        "df_features['BB_Squeeze'] = (df_features['BB_Width'] < df_features['BB_Width'].rolling(20).mean() * 0.8).astype(int)\n",
        "\n",
        "# === ATR (Volatility) ===\n",
        "df_features['High_Low'] = df_features['High'] - df_features['Low']\n",
        "df_features['High_Close'] = abs(df_features['High'] - df_features['Close'].shift())\n",
        "df_features['Low_Close'] = abs(df_features['Low'] - df_features['Close'].shift())\n",
        "df_features['True_Range'] = df_features[['High_Low', 'High_Close', 'Low_Close']].max(axis=1)\n",
        "df_features['ATR_14'] = df_features['True_Range'].rolling(window=14).mean()\n",
        "df_features['ATR_21'] = df_features['True_Range'].rolling(window=21).mean()\n",
        "\n",
        "# === PRICE-BASED FEATURES ===\n",
        "df_features['Price_Change'] = df_features['Close'].diff()\n",
        "df_features['Price_Change_Pct'] = df_features['Close'].pct_change() * 100\n",
        "df_features['High_Low_Pct'] = ((df_features['High'] - df_features['Low']) / df_features['Close']) * 100\n",
        "df_features['Open_Close_Pct'] = ((df_features['Close'] - df_features['Open']) / df_features['Open']) * 100\n",
        "df_features['Price_Position'] = (df_features['Close'] - df_features['Low']) / (df_features['High'] - df_features['Low'])\n",
        "\n",
        "# === VOLUME INDICATORS ===\n",
        "if 'Volume' in df_features.columns:\n",
        "    df_features['Volume_SMA_20'] = df_features['Volume'].rolling(window=20).mean()\n",
        "    df_features['Volume_Ratio'] = df_features['Volume'] / df_features['Volume_SMA_20']\n",
        "    df_features['Volume_Change'] = df_features['Volume'].pct_change()\n",
        "    df_features['Price_Volume_Trend'] = df_features['Price_Change_Pct'] * df_features['Volume_Ratio']\n",
        "\n",
        "# === MOMENTUM INDICATORS ===\n",
        "df_features['Momentum_5'] = df_features['Close'].pct_change(periods=5) * 100\n",
        "df_features['Momentum_10'] = df_features['Close'].pct_change(periods=10) * 100\n",
        "df_features['Momentum_20'] = df_features['Close'].pct_change(periods=20) * 100\n",
        "\n",
        "# === LAG FEATURES ===\n",
        "for lag in [1, 2, 3, 5, 10]:\n",
        "    df_features[f'Close_Lag_{lag}'] = df_features['Close'].shift(lag)\n",
        "    df_features[f'Return_Lag_{lag}'] = df_features['Price_Change_Pct'].shift(lag)\n",
        "\n",
        "# === ROLLING STATISTICS ===\n",
        "for window in [5, 10, 20]:\n",
        "    df_features[f'Rolling_Std_{window}'] = df_features['Close'].rolling(window=window).std()\n",
        "    df_features[f'Rolling_Mean_{window}'] = df_features['Close'].rolling(window=window).mean()\n",
        "    df_features[f'Rolling_Max_{window}'] = df_features['Close'].rolling(window=window).max()\n",
        "    df_features[f'Rolling_Min_{window}'] = df_features['Close'].rolling(window=window).min()\n",
        "\n",
        "print(f\"‚úÖ Technical indicators calculated!\")\n",
        "print(f\"   Total features: {len(df_features.columns)}\")\n",
        "print(f\"   New features added: {len(df_features.columns) - len(df.columns)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Target variables created:\n",
            "   - Target_Return: Next day return (regression)\n",
            "   - Target_Direction: Next day direction (binary classification)\n",
            "   - Target_Volatility: Next day volatility (regression)\n",
            "   - Target_Class: Multi-class movement (4 classes)\n",
            "   - Target_Return_5d, 10d, 20d: Multi-step ahead returns\n"
          ]
        }
      ],
      "source": [
        "# Create target variables for different prediction tasks\n",
        "\n",
        "# 1. Next day return (regression target)\n",
        "df_features['Target_Return'] = df_features['Close'].shift(-1) / df_features['Close'] - 1\n",
        "df_features['Target_Return_Pct'] = df_features['Target_Return'] * 100\n",
        "\n",
        "# 2. Next day price direction (classification target: 1 = up, 0 = down)\n",
        "df_features['Target_Direction'] = (df_features['Target_Return'] > 0).astype(int)\n",
        "\n",
        "# 3. Volatility prediction (next day ATR)\n",
        "df_features['Target_Volatility'] = df_features['ATR_14'].shift(-1)\n",
        "\n",
        "# 4. Multi-class classification (strong down, down, up, strong up)\n",
        "df_features['Target_Class'] = pd.cut(\n",
        "    df_features['Target_Return_Pct'],\n",
        "    bins=[-np.inf, -1, 0, 1, np.inf],\n",
        "    labels=[0, 1, 2, 3]  # 0=strong down, 1=down, 2=up, 3=strong up\n",
        ").astype(float)\n",
        "\n",
        "# 5. Future price (for multi-step ahead prediction)\n",
        "for horizon in [5, 10, 20]:\n",
        "    df_features[f'Target_Price_{horizon}d'] = df_features['Close'].shift(-horizon)\n",
        "    df_features[f'Target_Return_{horizon}d'] = (df_features[f'Target_Price_{horizon}d'] / df_features['Close'] - 1) * 100\n",
        "\n",
        "print(\"‚úÖ Target variables created:\")\n",
        "print(\"   - Target_Return: Next day return (regression)\")\n",
        "print(\"   - Target_Direction: Next day direction (binary classification)\")\n",
        "print(\"   - Target_Volatility: Next day volatility (regression)\")\n",
        "print(\"   - Target_Class: Multi-class movement (4 classes)\")\n",
        "print(\"   - Target_Return_5d, 10d, 20d: Multi-step ahead returns\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Handle Missing Values & Final Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Data cleaning:\n",
            "   Initial rows: 6,609\n",
            "   Final rows: 6,181\n",
            "   Removed: 428 rows (6.48%)\n",
            "   Date range: 2001-02-13 to 2025-12-18\n",
            "\n",
            "‚úÖ No missing values remaining!\n"
          ]
        }
      ],
      "source": [
        "# Remove rows with NaN values and handle infinity (from rolling windows and lag features)\n",
        "initial_rows = len(df_features)\n",
        "# Replace infinity with NaN, then drop\n",
        "df_features = df_features.replace([np.inf, -np.inf], np.nan)\n",
        "df_features = df_features.dropna().reset_index(drop=True)\n",
        "final_rows = len(df_features)\n",
        "\n",
        "print(f\"üìä Data cleaning:\")\n",
        "print(f\"   Initial rows: {initial_rows:,}\")\n",
        "print(f\"   Final rows: {final_rows:,}\")\n",
        "print(f\"   Removed: {initial_rows - final_rows:,} rows ({(initial_rows - final_rows)/initial_rows*100:.2f}%)\")\n",
        "print(f\"   Date range: {df_features['Date'].min().date()} to {df_features['Date'].max().date()}\")\n",
        "\n",
        "# Check for any remaining missing values\n",
        "remaining_missing = df_features.isnull().sum().sum()\n",
        "if remaining_missing > 0:\n",
        "    print(f\"\\n‚ö†Ô∏è  Warning: {remaining_missing} missing values still present\")\n",
        "    print(df_features.isnull().sum()[df_features.isnull().sum() > 0])\n",
        "else:\n",
        "    print(f\"\\n‚úÖ No missing values remaining!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Feature Selection & Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Feature Selection:\n",
            "   Total columns: 83\n",
            "   Feature columns: 70\n",
            "   Target columns: 11\n",
            "\n",
            "   Sample features: Open, High, Low, Close, Volume, Dividends, Stock Splits, SMA_5, SMA_10, SMA_20...\n",
            "\n",
            "   Targets: Target_Return, Target_Return_Pct, Target_Direction, Target_Volatility, Target_Class, Target_Price_5d, Target_Return_5d, Target_Price_10d, Target_Return_10d, Target_Price_20d, Target_Return_20d\n",
            "\n",
            "‚úÖ Feature matrices created:\n",
            "   X shape: (6181, 70)\n",
            "   y_return shape: (6181,)\n",
            "   y_direction shape: (6181,)\n"
          ]
        }
      ],
      "source": [
        "# Select features for modeling\n",
        "# Exclude Date, Ticker, and target variables from features\n",
        "exclude_cols = ['Date']\n",
        "if 'Ticker' in df_features.columns:\n",
        "    exclude_cols.append('Ticker')\n",
        "\n",
        "# Target variables\n",
        "target_cols = [col for col in df_features.columns if col.startswith('Target_')]\n",
        "\n",
        "# Feature columns (everything except dates, tickers, and targets)\n",
        "feature_cols = [col for col in df_features.columns if col not in exclude_cols + target_cols]\n",
        "\n",
        "print(f\"üìä Feature Selection:\")\n",
        "print(f\"   Total columns: {len(df_features.columns)}\")\n",
        "print(f\"   Feature columns: {len(feature_cols)}\")\n",
        "print(f\"   Target columns: {len(target_cols)}\")\n",
        "print(f\"\\n   Sample features: {', '.join(feature_cols[:10])}...\")\n",
        "print(f\"\\n   Targets: {', '.join(target_cols)}\")\n",
        "\n",
        "# Create feature matrix and targets\n",
        "X = df_features[feature_cols].copy()\n",
        "y_return = df_features['Target_Return'].copy()\n",
        "y_direction = df_features['Target_Direction'].copy()\n",
        "y_volatility = df_features['Target_Volatility'].copy()\n",
        "y_class = df_features['Target_Class'].copy()\n",
        "\n",
        "print(f\"\\n‚úÖ Feature matrices created:\")\n",
        "print(f\"   X shape: {X.shape}\")\n",
        "print(f\"   y_return shape: {y_return.shape}\")\n",
        "print(f\"   y_direction shape: {y_direction.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Feature Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Input X contains infinity or a value too large for dtype('float64').",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Scale features for better model performance\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Use StandardScaler for most features (mean=0, std=1)\u001b[39;00m\n\u001b[32m      3\u001b[39m scaler = StandardScaler()\n\u001b[32m      4\u001b[39m X_scaled = pd.DataFrame(\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m      6\u001b[39m     columns=X.columns,\n\u001b[32m      7\u001b[39m     index=X.index\n\u001b[32m      8\u001b[39m )\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Features scaled using StandardScaler\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Scaled X shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_scaled.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\sklearn\\base.py:907\u001b[39m, in \u001b[36mTransformerMixin.fit_transform\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    892\u001b[39m         warnings.warn(\n\u001b[32m    893\u001b[39m             (\n\u001b[32m    894\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) has a `transform`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    902\u001b[39m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m    903\u001b[39m         )\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    906\u001b[39m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m907\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m.transform(X)\n\u001b[32m    908\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    909\u001b[39m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[32m    910\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fit(X, y, **fit_params).transform(X)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:924\u001b[39m, in \u001b[36mStandardScaler.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[32m    923\u001b[39m \u001b[38;5;28mself\u001b[39m._reset()\n\u001b[32m--> \u001b[39m\u001b[32m924\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\sklearn\\base.py:1336\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m     estimator._validate_params()\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1332\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1333\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1334\u001b[39m     )\n\u001b[32m   1335\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:961\u001b[39m, in \u001b[36mStandardScaler.partial_fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    959\u001b[39m xp, _, X_device = get_namespace_and_device(X)\n\u001b[32m    960\u001b[39m first_call = \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mn_samples_seen_\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m961\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43msupported_float_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_device\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    969\u001b[39m n_features = X.shape[\u001b[32m1\u001b[39m]\n\u001b[32m    971\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\sklearn\\utils\\validation.py:2902\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2900\u001b[39m         out = X, y\n\u001b[32m   2901\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2902\u001b[39m     out = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2903\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2904\u001b[39m     out = _check_y(y, **check_params)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\sklearn\\utils\\validation.py:1074\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1068\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1069\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound array with dim \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray.ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1070\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m while dim <= 2 is required\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1071\u001b[39m     )\n\u001b[32m   1073\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[32m-> \u001b[39m\u001b[32m1074\u001b[39m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1075\u001b[39m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1078\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1079\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[32m   1082\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[32m   1083\u001b[39m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\sklearn\\utils\\validation.py:133\u001b[39m, in \u001b[36m_assert_all_finite\u001b[39m\u001b[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\sklearn\\utils\\validation.py:182\u001b[39m, in \u001b[36m_assert_all_finite_element_wise\u001b[39m\u001b[34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name == \u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[32m    168\u001b[39m     msg_err += (\n\u001b[32m    169\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not accept missing values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    170\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    180\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m#estimators-that-handle-nan-values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    181\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
            "\u001b[31mValueError\u001b[39m: Input X contains infinity or a value too large for dtype('float64')."
          ]
        }
      ],
      "source": [
        "# Scale features for better model performance\n",
        "# Use StandardScaler for most features (mean=0, std=1)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = pd.DataFrame(\n",
        "    scaler.fit_transform(X),\n",
        "    columns=X.columns,\n",
        "    index=X.index\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Features scaled using StandardScaler\")\n",
        "print(f\"   Scaled X shape: {X_scaled.shape}\")\n",
        "print(f\"\\n   Sample statistics (first 5 features):\")\n",
        "print(X_scaled.iloc[:, :5].describe().T[['mean', 'std', 'min', 'max']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Train-Test Split (Time Series Aware)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For time series, we split chronologically (not randomly)\n",
        "# Use 80% for training, 20% for testing\n",
        "split_idx = int(len(X_scaled) * 0.8)\n",
        "\n",
        "X_train = X_scaled.iloc[:split_idx].copy()\n",
        "X_test = X_scaled.iloc[split_idx:].copy()\n",
        "\n",
        "y_train_return = y_return.iloc[:split_idx].copy()\n",
        "y_test_return = y_return.iloc[split_idx:].copy()\n",
        "\n",
        "y_train_direction = y_direction.iloc[:split_idx].copy()\n",
        "y_test_direction = y_direction.iloc[split_idx:].copy()\n",
        "\n",
        "y_train_volatility = y_volatility.iloc[:split_idx].copy()\n",
        "y_test_volatility = y_volatility.iloc[split_idx:].copy()\n",
        "\n",
        "y_train_class = y_class.iloc[:split_idx].copy()\n",
        "y_test_class = y_class.iloc[split_idx:].copy()\n",
        "\n",
        "# Get date ranges\n",
        "train_dates = df_features['Date'].iloc[:split_idx]\n",
        "test_dates = df_features['Date'].iloc[split_idx:]\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"TRAIN-TEST SPLIT\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nüìä Training Set:\")\n",
        "print(f\"   Samples: {len(X_train):,}\")\n",
        "print(f\"   Date range: {train_dates.min().date()} to {train_dates.max().date()}\")\n",
        "print(f\"   Percentage: {len(X_train)/len(X_scaled)*100:.1f}%\")\n",
        "\n",
        "print(f\"\\nüìä Test Set:\")\n",
        "print(f\"   Samples: {len(X_test):,}\")\n",
        "print(f\"   Date range: {test_dates.min().date()} to {test_dates.max().date()}\")\n",
        "print(f\"   Percentage: {len(X_test)/len(X_scaled)*100:.1f}%\")\n",
        "\n",
        "print(f\"\\n‚úÖ Data split complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Save Processed Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save processed datasets\n",
        "print(\"üíæ Saving processed data...\")\n",
        "print(f\"   Saving to: {data_processed_dir_abs}\")\n",
        "\n",
        "# Ensure directory exists\n",
        "os.makedirs(data_processed_dir_abs, exist_ok=True)\n",
        "\n",
        "# Save full feature-engineered dataset (using absolute path)\n",
        "full_dataset_path = os.path.join(data_processed_dir_abs, \"bist_features_full.csv\")\n",
        "df_features.to_csv(full_dataset_path, index=False)\n",
        "print(f\"   ‚úÖ Full dataset: {full_dataset_path}\")\n",
        "\n",
        "# Save train/test splits (using absolute paths)\n",
        "X_train_path = os.path.join(data_processed_dir_abs, \"X_train.csv\")\n",
        "X_test_path = os.path.join(data_processed_dir_abs, \"X_test.csv\")\n",
        "X_train.to_csv(X_train_path, index=False)\n",
        "X_test.to_csv(X_test_path, index=False)\n",
        "print(f\"   ‚úÖ Feature matrices: X_train.csv, X_test.csv\")\n",
        "\n",
        "# Save targets (using absolute paths)\n",
        "targets_train = pd.DataFrame({\n",
        "    'Target_Return': y_train_return,\n",
        "    'Target_Direction': y_train_direction,\n",
        "    'Target_Volatility': y_train_volatility,\n",
        "    'Target_Class': y_train_class\n",
        "})\n",
        "targets_test = pd.DataFrame({\n",
        "    'Target_Return': y_test_return,\n",
        "    'Target_Direction': y_test_direction,\n",
        "    'Target_Volatility': y_test_volatility,\n",
        "    'Target_Class': y_test_class\n",
        "})\n",
        "\n",
        "y_train_path = os.path.join(data_processed_dir_abs, \"y_train.csv\")\n",
        "y_test_path = os.path.join(data_processed_dir_abs, \"y_test.csv\")\n",
        "targets_train.to_csv(y_train_path, index=False)\n",
        "targets_test.to_csv(y_test_path, index=False)\n",
        "print(f\"   ‚úÖ Target variables: y_train.csv, y_test.csv\")\n",
        "\n",
        "# Save feature names for reference (using absolute path)\n",
        "feature_info = pd.DataFrame({\n",
        "    'Feature_Name': feature_cols,\n",
        "    'Feature_Type': ['Technical' if any(x in col for x in ['SMA', 'EMA', 'RSI', 'MACD', 'BB', 'ATR']) \n",
        "                     else 'Price' if col in ['Open', 'High', 'Low', 'Close']\n",
        "                     else 'Volume' if 'Volume' in col\n",
        "                     else 'Derived' for col in feature_cols]\n",
        "})\n",
        "feature_info_path = os.path.join(data_processed_dir_abs, \"feature_info.csv\")\n",
        "feature_info.to_csv(feature_info_path, index=False)\n",
        "print(f\"   ‚úÖ Feature info: feature_info.csv\")\n",
        "\n",
        "# Verify files were created\n",
        "print(f\"\\nüìÇ Verifying saved files:\")\n",
        "saved_files = [\"bist_features_full.csv\", \"X_train.csv\", \"X_test.csv\", \"y_train.csv\", \"y_test.csv\", \"feature_info.csv\"]\n",
        "for file in saved_files:\n",
        "    file_path = os.path.join(data_processed_dir_abs, file)\n",
        "    if os.path.exists(file_path):\n",
        "        file_size = os.path.getsize(file_path) / 1024  # Size in KB\n",
        "        print(f\"   ‚úÖ {file} ({file_size:.2f} KB)\")\n",
        "    else:\n",
        "        print(f\"   ‚ùå {file} - NOT FOUND!\")\n",
        "\n",
        "print(f\"\\n‚úÖ All processed data saved to: {data_processed_dir_abs}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Summary & Next Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"PREPROCESSING SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nüìä Dataset Statistics:\")\n",
        "print(f\"   Original rows: {initial_rows:,}\")\n",
        "print(f\"   Processed rows: {len(df_features):,}\")\n",
        "print(f\"   Features created: {len(feature_cols)}\")\n",
        "print(f\"   Target variables: {len(target_cols)}\")\n",
        "\n",
        "print(f\"\\nüìà Feature Categories:\")\n",
        "feature_categories = {\n",
        "    'Moving Averages': [col for col in feature_cols if 'SMA' in col or 'EMA' in col],\n",
        "    'Momentum': [col for col in feature_cols if 'Momentum' in col or 'RSI' in col or 'MACD' in col],\n",
        "    'Volatility': [col for col in feature_cols if 'ATR' in col or 'Std' in col or 'BB' in col],\n",
        "    'Volume': [col for col in feature_cols if 'Volume' in col],\n",
        "    'Price': [col for col in feature_cols if col in ['Open', 'High', 'Low', 'Close']],\n",
        "    'Lags': [col for col in feature_cols if 'Lag' in col],\n",
        "    'Other': [col for col in feature_cols if col not in [item for sublist in [\n",
        "        [c for c in feature_cols if 'SMA' in c or 'EMA' in c],\n",
        "        [c for c in feature_cols if 'Momentum' in c or 'RSI' in c or 'MACD' in c],\n",
        "        [c for c in feature_cols if 'ATR' in c or 'Std' in c or 'BB' in c],\n",
        "        [c for c in feature_cols if 'Volume' in c],\n",
        "        [c for c in feature_cols if c in ['Open', 'High', 'Low', 'Close']],\n",
        "        [c for c in feature_cols if 'Lag' in c]\n",
        "    ] for item in sublist]]\n",
        "}\n",
        "\n",
        "for category, features in feature_categories.items():\n",
        "    if features:\n",
        "        print(f\"   {category}: {len(features)} features\")\n",
        "\n",
        "print(f\"\\nüéØ Target Variables:\")\n",
        "print(f\"   - Target_Return: Next day return (regression)\")\n",
        "print(f\"   - Target_Direction: Next day direction (binary classification)\")\n",
        "print(f\"   - Target_Volatility: Next day volatility (regression)\")\n",
        "print(f\"   - Target_Class: Multi-class movement (4 classes)\")\n",
        "\n",
        "print(f\"\\nüì¶ Data Files Saved:\")\n",
        "print(f\"   - bist_features_full.csv: Complete feature-engineered dataset\")\n",
        "print(f\"   - X_train.csv, X_test.csv: Scaled feature matrices\")\n",
        "print(f\"   - y_train.csv, y_test.csv: Target variables\")\n",
        "print(f\"   - feature_info.csv: Feature metadata\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ PREPROCESSING COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nüìã Next Steps:\")\n",
        "print(\"   1. Review feature distributions and correlations\")\n",
        "print(\"   2. Train machine learning models (regression & classification)\")\n",
        "print(\"   3. Evaluate model performance on test set\")\n",
        "print(\"   4. Feature importance analysis\")\n",
        "print(\"   5. Model tuning and optimization\")\n",
        "print(\"\\nüí° All processed data is ready for model training!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
