{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 3: Data Preprocessing & Feature Engineering\n",
        "\n",
        "## Goal\n",
        "Prepare BIST stock data for machine learning models by:\n",
        "- Cleaning and handling missing values\n",
        "- Engineering technical indicators and features\n",
        "- Creating target variables for prediction\n",
        "- Scaling and normalizing features\n",
        "- Splitting data for training/validation\n",
        "\n",
        "## Focus: BIST-100 Stock Analysis\n",
        "We'll work with the available stock data to create features for trend prediction, volatility forecasting, and price movement classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Preprocessing setup complete!\n",
            "   Project root: C:\\Users\\cihan\\turkish_finance_ml\n",
            "   Raw data dir: C:\\Users\\cihan\\turkish_finance_ml\\data\\raw\n",
            "   Processed data dir (absolute): C:\\Users\\cihan\\turkish_finance_ml\\data\\processed\n",
            "   Raw data exists: True\n",
            "   Stock file exists: True\n",
            "   Processed dir created: True\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import os\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "# Paths - Handle both notebook directory and project root\n",
        "# If running from notebooks/, go up one level; if from project root, use current\n",
        "current_dir = Path().resolve()\n",
        "if current_dir.name == \"notebooks\":\n",
        "    project_root = current_dir.parent\n",
        "else:\n",
        "    project_root = current_dir\n",
        "\n",
        "data_raw_dir = project_root / \"data\" / \"raw\"\n",
        "data_processed_dir = project_root / \"data\" / \"processed\"\n",
        "reports_dir = project_root / \"reports\"\n",
        "\n",
        "# Explicitly create directories using os.makedirs with absolute paths\n",
        "data_processed_dir_abs = str(data_processed_dir.resolve())\n",
        "reports_dir_abs = str(reports_dir.resolve())\n",
        "os.makedirs(data_processed_dir_abs, exist_ok=True)\n",
        "os.makedirs(reports_dir_abs, exist_ok=True)\n",
        "\n",
        "print(\"âœ… Preprocessing setup complete!\")\n",
        "print(f\"   Project root: {project_root}\")\n",
        "print(f\"   Raw data dir: {data_raw_dir}\")\n",
        "print(f\"   Processed data dir (absolute): {data_processed_dir_abs}\")\n",
        "print(f\"   Raw data exists: {data_raw_dir.exists()}\")\n",
        "print(f\"   Stock file exists: {(data_raw_dir / 'bist_stock_prices.csv').exists()}\")\n",
        "print(f\"   Processed dir created: {os.path.exists(data_processed_dir_abs)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Stock Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“‚ Looking for file: C:\\Users\\cihan\\turkish_finance_ml\\data\\raw\\bist_stock_prices.csv\n",
            "   File exists: True\n",
            "ğŸ“Š Loading data from: C:\\Users\\cihan\\turkish_finance_ml\\data\\raw\\bist_stock_prices.csv\n",
            "\n",
            "   Initial dataset shape: (62216, 9)\n",
            "   Available tickers: ['AKBNK.IS' 'ARCLK.IS' 'SASA.IS' 'THYAO.IS' 'GARAN.IS' 'SAHOL.IS'\n",
            " 'PETKM.IS' 'TUPRS.IS' 'BIMAS.IS' 'KOZAL.IS']\n",
            "âš ï¸  BIST-100 (XU100.IS) not found in data.\n",
            "   Available tickers: AKBNK.IS, ARCLK.IS, SASA.IS, THYAO.IS, GARAN.IS...\n",
            "   Using ticker with most data: SASA.IS (6610 records)\n",
            "\n",
            "   Final dataset shape: (6610, 9)\n",
            "   Date range: 2000-05-10 to 2026-01-16\n",
            "   Columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits', 'Ticker']\n"
          ]
        }
      ],
      "source": [
        "# Load BIST stock data\n",
        "stock_file = data_raw_dir / \"bist_stock_prices.csv\"\n",
        "\n",
        "print(f\"ğŸ“‚ Looking for file: {stock_file}\")\n",
        "print(f\"   File exists: {stock_file.exists()}\")\n",
        "\n",
        "if stock_file.exists():\n",
        "    print(f\"ğŸ“Š Loading data from: {stock_file}\")\n",
        "    df = pd.read_csv(stock_file)\n",
        "    df['Date'] = pd.to_datetime(df['Date'])\n",
        "    df = df.sort_values('Date').reset_index(drop=True)\n",
        "    \n",
        "    print(f\"\\n   Initial dataset shape: {df.shape}\")\n",
        "    if 'Ticker' in df.columns:\n",
        "        print(f\"   Available tickers: {df['Ticker'].unique()}\")\n",
        "    \n",
        "    # Filter for BIST-100 index if multiple tickers exist\n",
        "    if 'Ticker' in df.columns:\n",
        "        unique_tickers = df['Ticker'].unique()\n",
        "        if 'XU100.IS' in unique_tickers:\n",
        "            df = df[df['Ticker'] == 'XU100.IS'].copy().reset_index(drop=True)\n",
        "            print(f\"âœ… Loaded BIST-100 index data (XU100.IS)\")\n",
        "        else:\n",
        "            # Use first ticker if BIST-100 not available\n",
        "            # Count records per ticker and use the one with most data\n",
        "            ticker_counts = df['Ticker'].value_counts()\n",
        "            selected_ticker = ticker_counts.index[0]\n",
        "            df = df[df['Ticker'] == selected_ticker].copy().reset_index(drop=True)\n",
        "            print(f\"âš ï¸  BIST-100 (XU100.IS) not found in data.\")\n",
        "            print(f\"   Available tickers: {', '.join(unique_tickers[:5])}...\")\n",
        "            print(f\"   Using ticker with most data: {selected_ticker} ({ticker_counts[selected_ticker]} records)\")\n",
        "    else:\n",
        "        print(\"âœ… Loaded data (no ticker column - single stock/index)\")\n",
        "    \n",
        "    print(f\"\\n   Final dataset shape: {df.shape}\")\n",
        "    print(f\"   Date range: {df['Date'].min().date()} to {df['Date'].max().date()}\")\n",
        "    print(f\"   Columns: {df.columns.tolist()}\")\n",
        "else:\n",
        "    print(\"âŒ Error: bist_stock_prices.csv not found!\")\n",
        "    print(f\"   Expected location: {stock_file}\")\n",
        "    print(f\"   Current working directory: {Path.cwd()}\")\n",
        "    print(f\"   Project root: {project_root}\")\n",
        "    print(f\"   Raw data directory: {data_raw_dir}\")\n",
        "    print(f\"   Please ensure the file exists at the expected location.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Cleaning & Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "MISSING VALUES CHECK\n",
            "============================================================\n",
            "\n",
            "âœ… No missing values found!\n"
          ]
        }
      ],
      "source": [
        "# Check for missing values\n",
        "print(\"=\"*60)\n",
        "print(\"MISSING VALUES CHECK\")\n",
        "print(\"=\"*60)\n",
        "missing_count = df.isnull().sum()\n",
        "missing_pct = (missing_count / len(df)) * 100\n",
        "missing_df = pd.DataFrame({\n",
        "    'Column': missing_count.index,\n",
        "    'Missing Count': missing_count.values,\n",
        "    'Missing %': missing_pct.values\n",
        "})\n",
        "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
        "\n",
        "if len(missing_df) > 0:\n",
        "    print(\"\\nâš ï¸  Missing values found:\")\n",
        "    display(missing_df)\n",
        "    \n",
        "    # Forward fill for price data, backward fill for volume\n",
        "    price_cols = ['Open', 'High', 'Low', 'Close']\n",
        "    for col in price_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].fillna(method='ffill').fillna(method='bfill')\n",
        "    \n",
        "    if 'Volume' in df.columns:\n",
        "        df['Volume'] = df['Volume'].fillna(method='bfill').fillna(method='ffill')\n",
        "    \n",
        "    print(\"\\nâœ… Missing values handled (forward/backward fill)\")\n",
        "else:\n",
        "    print(\"\\nâœ… No missing values found!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Feature Engineering - Technical Indicators\n",
        "\n",
        "Creating comprehensive technical indicators for BIST-100 analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“Š Calculating technical indicators...\n",
            "âœ… Technical indicators calculated!\n",
            "   Total features: 72\n",
            "   New features added: 63\n"
          ]
        }
      ],
      "source": [
        "# Calculate comprehensive technical indicators\n",
        "df_features = df.copy()\n",
        "df_features = df_features.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "print(\"ğŸ“Š Calculating technical indicators...\")\n",
        "\n",
        "# === MOVING AVERAGES ===\n",
        "df_features['SMA_5'] = df_features['Close'].rolling(window=5).mean()\n",
        "df_features['SMA_10'] = df_features['Close'].rolling(window=10).mean()\n",
        "df_features['SMA_20'] = df_features['Close'].rolling(window=20).mean()\n",
        "df_features['SMA_50'] = df_features['Close'].rolling(window=50).mean()\n",
        "df_features['SMA_200'] = df_features['Close'].rolling(window=200).mean()\n",
        "\n",
        "# Exponential Moving Averages\n",
        "df_features['EMA_12'] = df_features['Close'].ewm(span=12, adjust=False).mean()\n",
        "df_features['EMA_26'] = df_features['Close'].ewm(span=26, adjust=False).mean()\n",
        "df_features['EMA_50'] = df_features['Close'].ewm(span=50, adjust=False).mean()\n",
        "\n",
        "# Moving Average Crossovers\n",
        "df_features['SMA_Cross_5_20'] = df_features['SMA_5'] - df_features['SMA_20']\n",
        "df_features['SMA_Cross_20_50'] = df_features['SMA_20'] - df_features['SMA_50']\n",
        "df_features['EMA_Cross'] = df_features['EMA_12'] - df_features['EMA_26']\n",
        "\n",
        "# === MACD ===\n",
        "df_features['MACD'] = df_features['EMA_12'] - df_features['EMA_26']\n",
        "df_features['MACD_Signal'] = df_features['MACD'].ewm(span=9, adjust=False).mean()\n",
        "df_features['MACD_Histogram'] = df_features['MACD'] - df_features['MACD_Signal']\n",
        "\n",
        "# === RSI ===\n",
        "def calculate_rsi(prices, period=14):\n",
        "    delta = prices.diff()\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
        "    rs = gain / loss\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "    return rsi\n",
        "\n",
        "df_features['RSI'] = calculate_rsi(df_features['Close'], period=14)\n",
        "df_features['RSI_Overbought'] = (df_features['RSI'] > 70).astype(int)\n",
        "df_features['RSI_Oversold'] = (df_features['RSI'] < 30).astype(int)\n",
        "\n",
        "# === BOLLINGER BANDS ===\n",
        "df_features['BB_Middle'] = df_features['Close'].rolling(window=20).mean()\n",
        "bb_std = df_features['Close'].rolling(window=20).std()\n",
        "df_features['BB_Upper'] = df_features['BB_Middle'] + (bb_std * 2)\n",
        "df_features['BB_Lower'] = df_features['BB_Middle'] - (bb_std * 2)\n",
        "df_features['BB_Width'] = df_features['BB_Upper'] - df_features['BB_Lower']\n",
        "df_features['BB_Position'] = (df_features['Close'] - df_features['BB_Lower']) / (df_features['BB_Upper'] - df_features['BB_Lower'])\n",
        "df_features['BB_Squeeze'] = (df_features['BB_Width'] < df_features['BB_Width'].rolling(20).mean() * 0.8).astype(int)\n",
        "\n",
        "# === ATR (Volatility) ===\n",
        "df_features['High_Low'] = df_features['High'] - df_features['Low']\n",
        "df_features['High_Close'] = abs(df_features['High'] - df_features['Close'].shift())\n",
        "df_features['Low_Close'] = abs(df_features['Low'] - df_features['Close'].shift())\n",
        "df_features['True_Range'] = df_features[['High_Low', 'High_Close', 'Low_Close']].max(axis=1)\n",
        "df_features['ATR_14'] = df_features['True_Range'].rolling(window=14).mean()\n",
        "df_features['ATR_21'] = df_features['True_Range'].rolling(window=21).mean()\n",
        "\n",
        "# === PRICE-BASED FEATURES ===\n",
        "df_features['Price_Change'] = df_features['Close'].diff()\n",
        "df_features['Price_Change_Pct'] = df_features['Close'].pct_change() * 100\n",
        "df_features['High_Low_Pct'] = ((df_features['High'] - df_features['Low']) / df_features['Close']) * 100\n",
        "df_features['Open_Close_Pct'] = ((df_features['Close'] - df_features['Open']) / df_features['Open']) * 100\n",
        "df_features['Price_Position'] = (df_features['Close'] - df_features['Low']) / (df_features['High'] - df_features['Low'])\n",
        "\n",
        "# === VOLUME INDICATORS ===\n",
        "if 'Volume' in df_features.columns:\n",
        "    df_features['Volume_SMA_20'] = df_features['Volume'].rolling(window=20).mean()\n",
        "    df_features['Volume_Ratio'] = df_features['Volume'] / df_features['Volume_SMA_20']\n",
        "    df_features['Volume_Change'] = df_features['Volume'].pct_change()\n",
        "    df_features['Price_Volume_Trend'] = df_features['Price_Change_Pct'] * df_features['Volume_Ratio']\n",
        "\n",
        "# === MOMENTUM INDICATORS ===\n",
        "df_features['Momentum_5'] = df_features['Close'].pct_change(periods=5) * 100\n",
        "df_features['Momentum_10'] = df_features['Close'].pct_change(periods=10) * 100\n",
        "df_features['Momentum_20'] = df_features['Close'].pct_change(periods=20) * 100\n",
        "\n",
        "# === LAG FEATURES ===\n",
        "for lag in [1, 2, 3, 5, 10]:\n",
        "    df_features[f'Close_Lag_{lag}'] = df_features['Close'].shift(lag)\n",
        "    df_features[f'Return_Lag_{lag}'] = df_features['Price_Change_Pct'].shift(lag)\n",
        "\n",
        "# === ROLLING STATISTICS ===\n",
        "for window in [5, 10, 20]:\n",
        "    df_features[f'Rolling_Std_{window}'] = df_features['Close'].rolling(window=window).std()\n",
        "    df_features[f'Rolling_Mean_{window}'] = df_features['Close'].rolling(window=window).mean()\n",
        "    df_features[f'Rolling_Max_{window}'] = df_features['Close'].rolling(window=window).max()\n",
        "    df_features[f'Rolling_Min_{window}'] = df_features['Close'].rolling(window=window).min()\n",
        "\n",
        "print(f\"âœ… Technical indicators calculated!\")\n",
        "print(f\"   Total features: {len(df_features.columns)}\")\n",
        "print(f\"   New features added: {len(df_features.columns) - len(df.columns)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Target variables created:\n",
            "   - Target_Return: Next day return (regression)\n",
            "   - Target_Direction: Next day direction (binary classification)\n",
            "   - Target_Volatility: Next day volatility (regression)\n",
            "   - Target_Class: Multi-class movement (4 classes)\n",
            "   - Target_Return_5d, 10d, 20d: Multi-step ahead returns\n"
          ]
        }
      ],
      "source": [
        "# Create target variables for different prediction tasks\n",
        "\n",
        "# 1. Next day return (regression target)\n",
        "df_features['Target_Return'] = df_features['Close'].shift(-1) / df_features['Close'] - 1\n",
        "df_features['Target_Return_Pct'] = df_features['Target_Return'] * 100\n",
        "\n",
        "# 2. Next day price direction (classification target: 1 = up, 0 = down)\n",
        "df_features['Target_Direction'] = (df_features['Target_Return'] > 0).astype(int)\n",
        "\n",
        "# 3. Volatility prediction (next day ATR)\n",
        "df_features['Target_Volatility'] = df_features['ATR_14'].shift(-1)\n",
        "\n",
        "# 4. Multi-class classification (strong down, down, up, strong up)\n",
        "df_features['Target_Class'] = pd.cut(\n",
        "    df_features['Target_Return_Pct'],\n",
        "    bins=[-np.inf, -1, 0, 1, np.inf],\n",
        "    labels=[0, 1, 2, 3]  # 0=strong down, 1=down, 2=up, 3=strong up\n",
        ").astype(float)\n",
        "\n",
        "# 5. Future price (for multi-step ahead prediction)\n",
        "for horizon in [5, 10, 20]:\n",
        "    df_features[f'Target_Price_{horizon}d'] = df_features['Close'].shift(-horizon)\n",
        "    df_features[f'Target_Return_{horizon}d'] = (df_features[f'Target_Price_{horizon}d'] / df_features['Close'] - 1) * 100\n",
        "\n",
        "print(\"âœ… Target variables created:\")\n",
        "print(\"   - Target_Return: Next day return (regression)\")\n",
        "print(\"   - Target_Direction: Next day direction (binary classification)\")\n",
        "print(\"   - Target_Volatility: Next day volatility (regression)\")\n",
        "print(\"   - Target_Class: Multi-class movement (4 classes)\")\n",
        "print(\"   - Target_Return_5d, 10d, 20d: Multi-step ahead returns\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Handle Missing Values & Final Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“Š Data cleaning:\n",
            "   Initial rows: 6,610\n",
            "   Final rows: 6,000\n",
            "   Removed: 610 rows (9.23%)\n",
            "   Date range: 2001-02-13 to 2025-12-18\n",
            "\n",
            "âœ… No missing values remaining!\n"
          ]
        }
      ],
      "source": [
        "# Remove rows with NaN values and handle infinity (from rolling windows and lag features)\n",
        "initial_rows = len(df_features)\n",
        "# Replace infinity with NaN, then drop\n",
        "df_features = df_features.replace([np.inf, -np.inf], np.nan)\n",
        "df_features = df_features.dropna().reset_index(drop=True)\n",
        "final_rows = len(df_features)\n",
        "\n",
        "print(f\"ğŸ“Š Data cleaning:\")\n",
        "print(f\"   Initial rows: {initial_rows:,}\")\n",
        "print(f\"   Final rows: {final_rows:,}\")\n",
        "print(f\"   Removed: {initial_rows - final_rows:,} rows ({(initial_rows - final_rows)/initial_rows*100:.2f}%)\")\n",
        "print(f\"   Date range: {df_features['Date'].min().date()} to {df_features['Date'].max().date()}\")\n",
        "\n",
        "# Check for any remaining missing values\n",
        "remaining_missing = df_features.isnull().sum().sum()\n",
        "if remaining_missing > 0:\n",
        "    print(f\"\\nâš ï¸  Warning: {remaining_missing} missing values still present\")\n",
        "    print(df_features.isnull().sum()[df_features.isnull().sum() > 0])\n",
        "else:\n",
        "    print(f\"\\nâœ… No missing values remaining!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Feature Selection & Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“Š Feature Selection:\n",
            "   Total columns: 83\n",
            "   Feature columns: 70\n",
            "   Target columns: 11\n",
            "\n",
            "   Sample features: Open, High, Low, Close, Volume, Dividends, Stock Splits, SMA_5, SMA_10, SMA_20...\n",
            "\n",
            "   Targets: Target_Return, Target_Return_Pct, Target_Direction, Target_Volatility, Target_Class, Target_Price_5d, Target_Return_5d, Target_Price_10d, Target_Return_10d, Target_Price_20d, Target_Return_20d\n",
            "\n",
            "âœ… Feature matrices created:\n",
            "   X shape: (6000, 70)\n",
            "   y_return shape: (6000,)\n",
            "   y_direction shape: (6000,)\n"
          ]
        }
      ],
      "source": [
        "# Select features for modeling\n",
        "# Exclude Date, Ticker, and target variables from features\n",
        "exclude_cols = ['Date']\n",
        "if 'Ticker' in df_features.columns:\n",
        "    exclude_cols.append('Ticker')\n",
        "\n",
        "# Target variables\n",
        "target_cols = [col for col in df_features.columns if col.startswith('Target_')]\n",
        "\n",
        "# Feature columns (everything except dates, tickers, and targets)\n",
        "feature_cols = [col for col in df_features.columns if col not in exclude_cols + target_cols]\n",
        "\n",
        "print(f\"ğŸ“Š Feature Selection:\")\n",
        "print(f\"   Total columns: {len(df_features.columns)}\")\n",
        "print(f\"   Feature columns: {len(feature_cols)}\")\n",
        "print(f\"   Target columns: {len(target_cols)}\")\n",
        "print(f\"\\n   Sample features: {', '.join(feature_cols[:10])}...\")\n",
        "print(f\"\\n   Targets: {', '.join(target_cols)}\")\n",
        "\n",
        "# Create feature matrix and targets\n",
        "X = df_features[feature_cols].copy()\n",
        "y_return = df_features['Target_Return'].copy()\n",
        "y_direction = df_features['Target_Direction'].copy()\n",
        "y_volatility = df_features['Target_Volatility'].copy()\n",
        "y_class = df_features['Target_Class'].copy()\n",
        "\n",
        "print(f\"\\nâœ… Feature matrices created:\")\n",
        "print(f\"   X shape: {X.shape}\")\n",
        "print(f\"   y_return shape: {y_return.shape}\")\n",
        "print(f\"   y_direction shape: {y_direction.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Feature Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Features scaled using StandardScaler\n",
            "   Scaled X shape: (6000, 70)\n",
            "\n",
            "   Sample statistics (first 5 features):\n",
            "                mean       std       min        max\n",
            "Open    3.789561e-17  1.000083 -0.441712   5.342258\n",
            "High    0.000000e+00  1.000083 -0.441225   5.257245\n",
            "Low     0.000000e+00  1.000083 -0.442309   5.136014\n",
            "Close  -3.789561e-17  1.000083 -0.441680   5.316476\n",
            "Volume -9.473903e-18  1.000083 -0.670187  12.023014\n"
          ]
        }
      ],
      "source": [
        "# Scale features for better model performance\n",
        "# Use StandardScaler for most features (mean=0, std=1)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = pd.DataFrame(\n",
        "    scaler.fit_transform(X),\n",
        "    columns=X.columns,\n",
        "    index=X.index\n",
        ")\n",
        "\n",
        "print(\"âœ… Features scaled using StandardScaler\")\n",
        "print(f\"   Scaled X shape: {X_scaled.shape}\")\n",
        "print(f\"\\n   Sample statistics (first 5 features):\")\n",
        "print(X_scaled.iloc[:, :5].describe().T[['mean', 'std', 'min', 'max']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Train-Test Split (Time Series Aware)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "TRAIN-TEST SPLIT\n",
            "============================================================\n",
            "\n",
            "ğŸ“Š Training Set:\n",
            "   Samples: 4,800\n",
            "   Date range: 2001-02-13 to 2021-01-19\n",
            "   Percentage: 80.0%\n",
            "\n",
            "ğŸ“Š Test Set:\n",
            "   Samples: 1,200\n",
            "   Date range: 2021-01-20 to 2025-12-18\n",
            "   Percentage: 20.0%\n",
            "\n",
            "âœ… Data split complete!\n"
          ]
        }
      ],
      "source": [
        "# For time series, we split chronologically (not randomly)\n",
        "# Use 80% for training, 20% for testing\n",
        "split_idx = int(len(X_scaled) * 0.8)\n",
        "\n",
        "X_train = X_scaled.iloc[:split_idx].copy()\n",
        "X_test = X_scaled.iloc[split_idx:].copy()\n",
        "\n",
        "y_train_return = y_return.iloc[:split_idx].copy()\n",
        "y_test_return = y_return.iloc[split_idx:].copy()\n",
        "\n",
        "y_train_direction = y_direction.iloc[:split_idx].copy()\n",
        "y_test_direction = y_direction.iloc[split_idx:].copy()\n",
        "\n",
        "y_train_volatility = y_volatility.iloc[:split_idx].copy()\n",
        "y_test_volatility = y_volatility.iloc[split_idx:].copy()\n",
        "\n",
        "y_train_class = y_class.iloc[:split_idx].copy()\n",
        "y_test_class = y_class.iloc[split_idx:].copy()\n",
        "\n",
        "# Get date ranges\n",
        "train_dates = df_features['Date'].iloc[:split_idx]\n",
        "test_dates = df_features['Date'].iloc[split_idx:]\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"TRAIN-TEST SPLIT\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nğŸ“Š Training Set:\")\n",
        "print(f\"   Samples: {len(X_train):,}\")\n",
        "print(f\"   Date range: {train_dates.min().date()} to {train_dates.max().date()}\")\n",
        "print(f\"   Percentage: {len(X_train)/len(X_scaled)*100:.1f}%\")\n",
        "\n",
        "print(f\"\\nğŸ“Š Test Set:\")\n",
        "print(f\"   Samples: {len(X_test):,}\")\n",
        "print(f\"   Date range: {test_dates.min().date()} to {test_dates.max().date()}\")\n",
        "print(f\"   Percentage: {len(X_test)/len(X_scaled)*100:.1f}%\")\n",
        "\n",
        "print(f\"\\nâœ… Data split complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Save Processed Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ’¾ Saving processed data...\n",
            "   Saving to: C:\\Users\\cihan\\turkish_finance_ml\\data\\processed\n",
            "   âœ… Full dataset: C:\\Users\\cihan\\turkish_finance_ml\\data\\processed\\bist_features_full.csv\n",
            "   âœ… Feature matrices: X_train.csv, X_test.csv\n",
            "   âœ… Target variables: y_train.csv, y_test.csv\n",
            "   âœ… Feature info: feature_info.csv\n",
            "\n",
            "ğŸ“‚ Verifying saved files:\n",
            "   âœ… bist_features_full.csv (8586.70 KB)\n",
            "   âœ… X_train.csv (6685.06 KB)\n",
            "   âœ… X_test.csv (1596.40 KB)\n",
            "   âœ… y_train.csv (218.72 KB)\n",
            "   âœ… y_test.csv (55.91 KB)\n",
            "   âœ… feature_info.csv (1.46 KB)\n",
            "\n",
            "âœ… All processed data saved to: C:\\Users\\cihan\\turkish_finance_ml\\data\\processed\n"
          ]
        }
      ],
      "source": [
        "# Save processed datasets\n",
        "print(\"ğŸ’¾ Saving processed data...\")\n",
        "print(f\"   Saving to: {data_processed_dir_abs}\")\n",
        "\n",
        "# Ensure directory exists\n",
        "os.makedirs(data_processed_dir_abs, exist_ok=True)\n",
        "\n",
        "# Save full feature-engineered dataset (using absolute path)\n",
        "full_dataset_path = os.path.join(data_processed_dir_abs, \"bist_features_full.csv\")\n",
        "df_features.to_csv(full_dataset_path, index=False)\n",
        "print(f\"   âœ… Full dataset: {full_dataset_path}\")\n",
        "\n",
        "# Save train/test splits (using absolute paths)\n",
        "X_train_path = os.path.join(data_processed_dir_abs, \"X_train.csv\")\n",
        "X_test_path = os.path.join(data_processed_dir_abs, \"X_test.csv\")\n",
        "X_train.to_csv(X_train_path, index=False)\n",
        "X_test.to_csv(X_test_path, index=False)\n",
        "print(f\"   âœ… Feature matrices: X_train.csv, X_test.csv\")\n",
        "\n",
        "# Save targets (using absolute paths)\n",
        "targets_train = pd.DataFrame({\n",
        "    'Target_Return': y_train_return,\n",
        "    'Target_Direction': y_train_direction,\n",
        "    'Target_Volatility': y_train_volatility,\n",
        "    'Target_Class': y_train_class\n",
        "})\n",
        "targets_test = pd.DataFrame({\n",
        "    'Target_Return': y_test_return,\n",
        "    'Target_Direction': y_test_direction,\n",
        "    'Target_Volatility': y_test_volatility,\n",
        "    'Target_Class': y_test_class\n",
        "})\n",
        "\n",
        "y_train_path = os.path.join(data_processed_dir_abs, \"y_train.csv\")\n",
        "y_test_path = os.path.join(data_processed_dir_abs, \"y_test.csv\")\n",
        "targets_train.to_csv(y_train_path, index=False)\n",
        "targets_test.to_csv(y_test_path, index=False)\n",
        "print(f\"   âœ… Target variables: y_train.csv, y_test.csv\")\n",
        "\n",
        "# Save feature names for reference (using absolute path)\n",
        "feature_info = pd.DataFrame({\n",
        "    'Feature_Name': feature_cols,\n",
        "    'Feature_Type': ['Technical' if any(x in col for x in ['SMA', 'EMA', 'RSI', 'MACD', 'BB', 'ATR']) \n",
        "                     else 'Price' if col in ['Open', 'High', 'Low', 'Close']\n",
        "                     else 'Volume' if 'Volume' in col\n",
        "                     else 'Derived' for col in feature_cols]\n",
        "})\n",
        "feature_info_path = os.path.join(data_processed_dir_abs, \"feature_info.csv\")\n",
        "feature_info.to_csv(feature_info_path, index=False)\n",
        "print(f\"   âœ… Feature info: feature_info.csv\")\n",
        "\n",
        "# Verify files were created\n",
        "print(f\"\\nğŸ“‚ Verifying saved files:\")\n",
        "saved_files = [\"bist_features_full.csv\", \"X_train.csv\", \"X_test.csv\", \"y_train.csv\", \"y_test.csv\", \"feature_info.csv\"]\n",
        "for file in saved_files:\n",
        "    file_path = os.path.join(data_processed_dir_abs, file)\n",
        "    if os.path.exists(file_path):\n",
        "        file_size = os.path.getsize(file_path) / 1024  # Size in KB\n",
        "        print(f\"   âœ… {file} ({file_size:.2f} KB)\")\n",
        "    else:\n",
        "        print(f\"   âŒ {file} - NOT FOUND!\")\n",
        "\n",
        "print(f\"\\nâœ… All processed data saved to: {data_processed_dir_abs}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Summary & Next Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "PREPROCESSING SUMMARY\n",
            "============================================================\n",
            "\n",
            "ğŸ“Š Dataset Statistics:\n",
            "   Original rows: 6,610\n",
            "   Processed rows: 6,000\n",
            "   Features created: 70\n",
            "   Target variables: 11\n",
            "\n",
            "ğŸ“ˆ Feature Categories:\n",
            "   Moving Averages: 12 features\n",
            "   Momentum: 9 features\n",
            "   Volatility: 11 features\n",
            "   Volume: 5 features\n",
            "   Price: 4 features\n",
            "   Lags: 10 features\n",
            "   Other: 20 features\n",
            "\n",
            "ğŸ¯ Target Variables:\n",
            "   - Target_Return: Next day return (regression)\n",
            "   - Target_Direction: Next day direction (binary classification)\n",
            "   - Target_Volatility: Next day volatility (regression)\n",
            "   - Target_Class: Multi-class movement (4 classes)\n",
            "\n",
            "ğŸ“¦ Data Files Saved:\n",
            "   - bist_features_full.csv: Complete feature-engineered dataset\n",
            "   - X_train.csv, X_test.csv: Scaled feature matrices\n",
            "   - y_train.csv, y_test.csv: Target variables\n",
            "   - feature_info.csv: Feature metadata\n",
            "\n",
            "============================================================\n",
            "âœ… PREPROCESSING COMPLETE!\n",
            "============================================================\n",
            "\n",
            "ğŸ“‹ Next Steps:\n",
            "   1. Review feature distributions and correlations\n",
            "   2. Train machine learning models (regression & classification)\n",
            "   3. Evaluate model performance on test set\n",
            "   4. Feature importance analysis\n",
            "   5. Model tuning and optimization\n",
            "\n",
            "ğŸ’¡ All processed data is ready for model training!\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"PREPROCESSING SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nğŸ“Š Dataset Statistics:\")\n",
        "print(f\"   Original rows: {initial_rows:,}\")\n",
        "print(f\"   Processed rows: {len(df_features):,}\")\n",
        "print(f\"   Features created: {len(feature_cols)}\")\n",
        "print(f\"   Target variables: {len(target_cols)}\")\n",
        "\n",
        "print(f\"\\nğŸ“ˆ Feature Categories:\")\n",
        "feature_categories = {\n",
        "    'Moving Averages': [col for col in feature_cols if 'SMA' in col or 'EMA' in col],\n",
        "    'Momentum': [col for col in feature_cols if 'Momentum' in col or 'RSI' in col or 'MACD' in col],\n",
        "    'Volatility': [col for col in feature_cols if 'ATR' in col or 'Std' in col or 'BB' in col],\n",
        "    'Volume': [col for col in feature_cols if 'Volume' in col],\n",
        "    'Price': [col for col in feature_cols if col in ['Open', 'High', 'Low', 'Close']],\n",
        "    'Lags': [col for col in feature_cols if 'Lag' in col],\n",
        "    'Other': [col for col in feature_cols if col not in [item for sublist in [\n",
        "        [c for c in feature_cols if 'SMA' in c or 'EMA' in c],\n",
        "        [c for c in feature_cols if 'Momentum' in c or 'RSI' in c or 'MACD' in c],\n",
        "        [c for c in feature_cols if 'ATR' in c or 'Std' in c or 'BB' in c],\n",
        "        [c for c in feature_cols if 'Volume' in c],\n",
        "        [c for c in feature_cols if c in ['Open', 'High', 'Low', 'Close']],\n",
        "        [c for c in feature_cols if 'Lag' in c]\n",
        "    ] for item in sublist]]\n",
        "}\n",
        "\n",
        "for category, features in feature_categories.items():\n",
        "    if features:\n",
        "        print(f\"   {category}: {len(features)} features\")\n",
        "\n",
        "print(f\"\\nğŸ¯ Target Variables:\")\n",
        "print(f\"   - Target_Return: Next day return (regression)\")\n",
        "print(f\"   - Target_Direction: Next day direction (binary classification)\")\n",
        "print(f\"   - Target_Volatility: Next day volatility (regression)\")\n",
        "print(f\"   - Target_Class: Multi-class movement (4 classes)\")\n",
        "\n",
        "print(f\"\\nğŸ“¦ Data Files Saved:\")\n",
        "print(f\"   - bist_features_full.csv: Complete feature-engineered dataset\")\n",
        "print(f\"   - X_train.csv, X_test.csv: Scaled feature matrices\")\n",
        "print(f\"   - y_train.csv, y_test.csv: Target variables\")\n",
        "print(f\"   - feature_info.csv: Feature metadata\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ… PREPROCESSING COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nğŸ“‹ Next Steps:\")\n",
        "print(\"   1. Review feature distributions and correlations\")\n",
        "print(\"   2. Train machine learning models (regression & classification)\")\n",
        "print(\"   3. Evaluate model performance on test set\")\n",
        "print(\"   4. Feature importance analysis\")\n",
        "print(\"   5. Model tuning and optimization\")\n",
        "print(\"\\nğŸ’¡ All processed data is ready for model training!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
